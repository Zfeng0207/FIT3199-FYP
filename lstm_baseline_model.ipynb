{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zfeng0207/FIT3199-FYP/blob/dev%2Fzfeng/lstm_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhFE7r-5ThiE"
      },
      "source": [
        "# Loading Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0CUDImu9MEW",
        "outputId": "96dbf4f9-1a7e-471a-ec40-b722dea7e85e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GfQLwvb-9N2C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/ECG-MIMIC-main')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TPU has a different way of downloading dependencies keep the cell below to avoid dependency conflicts"
      ],
      "metadata": {
        "id": "w2hkDlkQacc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K_PGqhC8-C_A",
        "outputId": "73aef3b6-0fc4-48b2-cf7e-43506b527b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/961.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.3/961.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qqqq mlflow torchmetrics pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vAG6zZC2VOKT"
      },
      "outputs": [],
      "source": [
        "import mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lgFOPav48kyv"
      },
      "outputs": [],
      "source": [
        "memmap_meta_path = \"src/data/memmap/memmap_meta.npz\"\n",
        "memmap_path = \"src/data/memmap/memmap.npy\"\n",
        "df_mapped_path = \"src/preprocessed_data/records_w_stroke_labels.csv\"\n",
        "df_pkl_path = \"src/preprocessed_data/records_w_diag.pkl\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge dataset with labels and ecg paths"
      ],
      "metadata": {
        "id": "X2qs7_d19187"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BgqmAcyf8kyv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_pkl = pd.read_pickle(df_pkl_path)\n",
        "df_mapped = pd.read_csv(df_mapped_path)  # Assuming df_mapped is saved as a pickle\n",
        "\n",
        "merged_df = pd.merge(df_pkl, df_mapped, on=[\"study_id\"], how=\"left\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "meta = np.load(memmap_meta_path, allow_pickle=True)\n",
        "filenames = meta['filenames']\n",
        "num_files = len(filenames)\n",
        "print(f\"Number of files: {num_files}\")"
      ],
      "metadata": {
        "id": "g3rsl9hgTq_L",
        "outputId": "3cefbd2d-7ed8-4d88-acab-46d8b2595114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(meta.files)"
      ],
      "metadata": {
        "id": "GzLESG1GT-YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pkl.shape"
      ],
      "metadata": {
        "id": "blABwqYbTjun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.shape"
      ],
      "metadata": {
        "id": "RSmBk5XVTgxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labeling stroke classes"
      ],
      "metadata": {
        "id": "4-YjkIhw8v1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_labels = df_mapped['label_test'].apply(lambda x: 0 if x == '[]' else 1).to_frame(name='label_test_binary')"
      ],
      "metadata": {
        "id": "9_JwqAJG8uZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing target class distribution"
      ],
      "metadata": {
        "id": "hXMaGa8y8zdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s4Go55w8kyx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Count the occurrences of each target class\n",
        "target_counts = df_labels['Stroke_YN'].value_counts()\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=target_counts.index, y=target_counts.values, palette=\"viridis\")\n",
        "plt.title(\"Target Distribution (Stroke_YN)\", fontsize=14)\n",
        "plt.xlabel(\"Stroke Y/N (0 = No Stroke, 1 = Stroke)\", fontsize=12)\n",
        "plt.ylabel(\"Count\", fontsize=12)\n",
        "plt.xticks([0, 1], labels=[\"No Stroke (0)\", \"Stroke (1)\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Mlflow for model baseline tracking"
      ],
      "metadata": {
        "id": "NXJk0klE85pI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtckZD2F8kyy"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import os\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"Zfeng0207\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"af7c8365aec4d3ff7a40563a35ec94d4bc9b4512\"\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"stroke-prediction-dagshub-repo\"\n",
        "\n",
        "experiment_name = \"ecg-lstm-experiment\"\n",
        "mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"MLflow tracking experiment name: {experiment_name}\")\n",
        "print(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzfifZa58kyz"
      },
      "source": [
        "# Data Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, memmap, starts, lengths, labels_df):\n",
        "        self.memmap = memmap\n",
        "        self.starts = starts\n",
        "        self.lengths = lengths\n",
        "        self.labels = labels_df['stroke_yn'].values\n",
        "        self.indices = labels_df.index.values  # align with memmap meta\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = self.starts[self.indices[idx]]\n",
        "        length = self.lengths[self.indices[idx]]\n",
        "        signal = self.memmap[start:start+length]\n",
        "        label = self.labels[idx]\n",
        "        return torch.tensor(signal, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "_xIO7FzUX3tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXSyMCeb8kyz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class ECGDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, memmap, starts, lengths, train_df, val_df, test_df, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.memmap = memmap\n",
        "        self.starts = starts\n",
        "        self.lengths = lengths\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.test_df = test_df\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = ECGDataset(self.memmap, self.starts, self.lengths, self.train_df)\n",
        "        self.val_dataset = ECGDataset(self.memmap, self.starts, self.lengths, self.val_df)\n",
        "        self.test_dataset = ECGDataset(self.memmap, self.starts, self.lengths, self.test_df)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyK98BI28ky0"
      },
      "source": [
        "# Simple LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aIji1CIf8ky0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMSleepClassifier(pl.LightningModule):\n",
        "    def __init__(self, hparams, input_size=12, hidden_size=64, num_layers=2, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        # Define metrics\n",
        "        self.train_f1 = BinaryF1Score()\n",
        "        self.val_f1 = BinaryF1Score()\n",
        "        self.test_f1 = BinaryF1Score()\n",
        "\n",
        "        self.train_auc = BinaryAUROC()\n",
        "        self.val_auc = BinaryAUROC()\n",
        "        self.test_auc = BinaryAUROC()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * 2, 1)  # bidirectional\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C) → needs to be (B, T, 12)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]  # take last timestep\n",
        "        logits = self.fc(out)\n",
        "        return logits.squeeze()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        preds = torch.sigmoid(logits) > 0.5\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.train_f1.update(preds, y)\n",
        "        self.train_auc.update(preds, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.log(\"train_acc\", acc)\n",
        "        return loss, acc\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        preds = torch.sigmoid(logits) > 0.5\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.val_f1.update(preds, y)\n",
        "        self.val_auc.update(preds, y)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.log(\"val_acc\", acc)\n",
        "        return loss, acc\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        test_loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
        "        preds = torch.sigmoid(logits) > 0.5\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.test_f1.update(preds, y)\n",
        "        self.test_auc.update(preds, y)\n",
        "        self.log(\"test_acc\", acc)\n",
        "        self.log(\"test_loss\", test_loss)\n",
        "\n",
        "        return test_loss, acc\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Sampling"
      ],
      "metadata": {
        "id": "mZrIbV31ciXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load metadata\n",
        "meta = np.load(\"memmap.npz\", allow_pickle=True)\n",
        "start = meta[\"start\"]\n",
        "length = meta[\"length\"]\n",
        "file_idx = meta[\"file_idx\"]\n",
        "filenames = meta[\"filenames\"]\n",
        "\n",
        "# Load labels CSV\n",
        "df = df_labels.copy()\n",
        "\n",
        "# Sanity check\n",
        "assert len(df) == len(start), \"Mismatch between label and memmap metadata length\"\n",
        "\n",
        "# Add metadata into DataFrame\n",
        "df['start'] = start\n",
        "df['length'] = length\n",
        "df['file_idx'] = file_idx\n",
        "\n",
        "# Now you can split the DataFrame while keeping track of ECG data pointers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split test set with preserved stroke ratio\n",
        "train_val_df, test_df = train_test_split(\n",
        "    df, test_size=0.10, stratify=df['stroke_yn'], random_state=42\n",
        ")\n",
        "\n",
        "# Then split stroke/non-stroke from train_val_df as discussed before\n",
        "stroke_df = train_val_df[train_val_df['stroke_yn'] == 1]\n",
        "nonstroke_df = train_val_df[train_val_df['stroke_yn'] == 0]\n",
        "\n",
        "# Balanced sampling\n",
        "train_stroke, val_stroke = train_test_split(stroke_df, test_size=0.1, random_state=42)\n",
        "train_nonstroke = nonstroke_df.sample(n=len(train_stroke)*2, random_state=42)\n",
        "val_nonstroke = nonstroke_df.drop(train_nonstroke.index).sample(n=len(val_stroke)*2, random_state=42)\n",
        "\n",
        "# Final splits\n",
        "train_df = pd.concat([train_stroke, train_nonstroke]).reset_index(drop=True)\n",
        "val_df = pd.concat([val_stroke, val_nonstroke]).reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "y5ip54F3cEWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model and data Initialization"
      ],
      "metadata": {
        "id": "UTqbdVzwYHao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9MH-kAG8ky0"
      },
      "outputs": [],
      "source": [
        "memmap_data = np.load(memmap_path, allow_pickle=True)\n",
        "starts = meta['start']\n",
        "lengths = meta['length']\n",
        "\n",
        "# Create the data module\n",
        "ecg_dm = ECGDataModule(\n",
        "    memmap=memmap_data,\n",
        "    starts=starts,\n",
        "    lengths=lengths,\n",
        "    train_df=train_df,\n",
        "    val_df=val_df,\n",
        "    test_df=test_df,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "model = LSTMSleepClassifier(input_size=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAqMr6bZ8ky0"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7bJCm-e8ky1"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import MLFlowLogger\n",
        "import mlflow\n",
        "\n",
        "# Optional: set MLflow tracking URI and experiment\n",
        "# mlflow.set_tracking_uri(\"file:./mlruns\")  # or your remote tracking URI\n",
        "# mlflow.set_experiment(\"StrokePredictionECG\")\n",
        "\n",
        "mlf_logger = MLFlowLogger(\n",
        "    experiment_name=\"ecg-lstm-experiment\",\n",
        "    # tracking_uri=\"file:./mlruns\",  # optional if already set globally\n",
        "    log_model=True  # logs the model checkpoint as artifact\n",
        ")\n",
        "\n",
        "# Pass the logger to the Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=10,\n",
        "    accelerator=\"auto\",\n",
        "    log_every_n_steps=10,\n",
        "    deterministic=True,\n",
        "    logger=mlf_logger,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics"
      ],
      "metadata": {
        "id": "l3XBvrv0YOqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.classification import BinaryF1Score, BinaryAUROC\n",
        "\n",
        "self.f1 = BinaryF1Score()\n",
        "self.auroc = BinaryAUROC()\n",
        "\n",
        "# In validation_step:\n",
        "self.f1(preds, y)\n",
        "self.auroc(preds.float(), y)\n"
      ],
      "metadata": {
        "id": "lViYrQ9aYOFn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}