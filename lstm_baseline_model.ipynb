{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zfeng0207/FIT3199-FYP/blob/dev%2Fzfeng/lstm_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting the Notebook\n",
        "\n"
      ],
      "metadata": {
        "id": "XE77-1DFUpTz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhFE7r-5ThiE"
      },
      "source": [
        "## Loading Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0CUDImu9MEW",
        "outputId": "c572778d-e02c-400a-afaa-cb05385c1360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GfQLwvb-9N2C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/ECG-MIMIC-main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K_PGqhC8-C_A"
      },
      "outputs": [],
      "source": [
        "# !pip install -qqqq mlflow torchmetrics pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vAG6zZC2VOKT"
      },
      "outputs": [],
      "source": [
        "import mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lgFOPav48kyv"
      },
      "outputs": [],
      "source": [
        "memmap_meta_path = \"src/data/memmap/memmap_meta.npz\"\n",
        "memmap_path = \"src/data/memmap/memmap.npy\"\n",
        "df_diag_path = \"src/data/records_w_diag_icd10.csv\"\n",
        "df_memmap_pkl_path = \"src/data/memmap/df_memmap.pkl\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge dataset with labels and ecg paths"
      ],
      "metadata": {
        "id": "X2qs7_d19187"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgqmAcyf8kyv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_diag = pd.read_csv(df_diag_path)\n",
        "df_mapped = pd.read_pickle(df_memmap_pkl_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merged_df = pd.merge(df_pkl, df_mapped, on=[\"study_id\"], how=\"left\")"
      ],
      "metadata": {
        "id": "RV9uV_TjhP8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labeling stroke classes"
      ],
      "metadata": {
        "id": "4-YjkIhw8v1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_labels = df_diag['label_test'].apply(lambda x: 0 if x == '[]' else 1).to_frame(name='Stroke_YN')"
      ],
      "metadata": {
        "id": "9_JwqAJG8uZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_labels.to_csv(\"src/data/labels.csv\")"
      ],
      "metadata": {
        "id": "KzfzVV_NT_dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_labels = pd.read_csv(\"src/data/labels.csv\")"
      ],
      "metadata": {
        "id": "DZqAwbgSUFwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_memmap_pkl_path = \"src/data/memmap/df_memmap.pkl\"\n",
        "df_memmap = pd.read_pickle(df_memmap_pkl_path)"
      ],
      "metadata": {
        "id": "_aknJMWv3mnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_memmap.head()"
      ],
      "metadata": {
        "id": "aP01Tks231Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing target class distribution"
      ],
      "metadata": {
        "id": "hXMaGa8y8zdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each target class\n",
        "target_counts = df_labels['Stroke_YN'].value_counts()\n",
        "print(target_counts)"
      ],
      "metadata": {
        "id": "Zkw2IgjXiodd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s4Go55w8kyx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=target_counts.index, y=target_counts.values, palette=\"viridis\")\n",
        "plt.title(\"Target Distribution (Stroke_YN)\", fontsize=14)\n",
        "plt.xlabel(\"Stroke Y/N (0 = No Stroke, 1 = Stroke)\", fontsize=12)\n",
        "plt.ylabel(\"Count\", fontsize=12)\n",
        "plt.xticks([0, 1], labels=[\"No Stroke (0)\", \"Stroke (1)\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing ECG Data"
      ],
      "metadata": {
        "id": "xigHrMS4yVNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"src/data/df_memmap.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "eOGPbUc43Hos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "memmap_meta = np.load(memmap_meta_path, allow_pickle=True)\n",
        "memmap_data = np.memmap(memmap_path, dtype=np.float32, mode='r')\n",
        "\n",
        "starts = memmap_meta[\"start\"]\n",
        "lengths = memmap_meta[\"length\"]\n",
        "original_shape = tuple(memmap_meta[\"shape\"][0])\n",
        "print(f\"Original shape from metadata: {original_shape}\")\n",
        "print(f\"Number of individual recordings: {len(starts)}\")\n",
        "print(memmap_data.shape)\n",
        "# Reshape data according to metadata\n",
        "ecg_data = memmap_data.reshape(original_shape)\n",
        "print(f\"Reshaped ECG data: {ecg_data.shape}\")\n",
        "\n",
        "# Function to visualize a 12-lead ECG\n",
        "def visualize_12lead_ecg(ecg_data, patient_index=0):\n",
        "    # Get the start and length for this patient\n",
        "    start_idx = starts[patient_index]\n",
        "    length = lengths[patient_index]\n",
        "\n",
        "    # Extract the data for this patient - all 12 leads\n",
        "    patient_data = ecg_data[start_idx:start_idx+length, :]\n",
        "\n",
        "    # Standard 12-lead ECG lead names\n",
        "    lead_names = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
        "\n",
        "    # Create a figure with 12 subplots (3x4 grid)\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Plot each lead\n",
        "    for i, ax in enumerate(axes):\n",
        "        if i < 12:  # We have 12 leads\n",
        "            ax.plot(patient_data[:, i])\n",
        "            ax.set_title(f'Lead {lead_names[i]}')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Add a small vertical scale bar (1 mV)\n",
        "            # This is an approximation - normally would need calibration\n",
        "            y_range = np.max(patient_data[:, i]) - np.min(patient_data[:, i])\n",
        "            scale_bar = y_range * 0.2  # 20% of the range as a scale reference\n",
        "            ax.plot([10, 10], [np.min(patient_data[:, i]), np.min(patient_data[:, i]) + scale_bar],\n",
        "                   'k-', linewidth=2)\n",
        "\n",
        "            # Remove tick labels to mimic clinical ECG appearance\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "        else:\n",
        "            ax.axis('off')  # Hide unused subplot\n",
        "\n",
        "    # Add a title for the entire plot\n",
        "    plt.suptitle(f'12-Lead ECG - Patient #{patient_index+1}', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.92)  # Adjust to make room for suptitle\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Visualize ECGs for first 3 patients\n",
        "for i in range(3):\n",
        "    fig = visualize_12lead_ecg(ecg_data, i)\n",
        "    plt.figure(fig.number)\n",
        "    plt.savefig(f'patient_{i+1}_12lead_ecg.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Basic statistics\n",
        "# print(\"\\nStatistics for all ECG data:\")\n",
        "# print(\"Min value:\", ecg_data.min())\n",
        "# print(\"Max value:\", ecg_data.max())\n",
        "# print(\"Mean:\", ecg_data.mean())\n",
        "# print(\"Standard deviation:\", ecg_data.std())"
      ],
      "metadata": {
        "id": "XoflBAfFyYLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzfifZa58kyz"
      },
      "source": [
        "## Data Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, memmap, labels_df):\n",
        "        self.df = labels_df\n",
        "        self.memmap = memmap\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # Access data directly from the DataFrame\n",
        "      start = self.df.loc[idx, 'start']\n",
        "      length = self.df.loc[idx, 'length']\n",
        "      # file_idx = self.df.loc[idx, 'file_idx'] # You might not need file_idx here anymore\n",
        "\n",
        "      # Extract the flat signal slice\n",
        "      signal = self.memmap[start : start + length * 12]  # 12 features per timestep\n",
        "      signal = (signal - signal.mean(axis=0)) / (signal.std(axis=0) + 1e-6)\n",
        "\n",
        "      # Reshape to [length, 12]\n",
        "      signal = signal.reshape(length, 12)\n",
        "\n",
        "      # Convert signal to PyTorch tensor before checking for NaN/inf\n",
        "      signal = torch.tensor(signal, dtype=torch.float32)\n",
        "\n",
        "      if torch.isnan(signal).any() or torch.isinf(signal).any():\n",
        "        return None\n",
        "\n",
        "      label = self.df.loc[idx, 'Stroke_YN']  # Access label from DataFrame\n",
        "      return signal, torch.tensor(label, dtype=torch.long) # signal is already a tensor\n"
      ],
      "metadata": {
        "id": "_xIO7FzUX3tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Swish"
      ],
      "metadata": {
        "id": "QPgELME87YIO"
      }
    },
    {
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "\n",
        "class Swish(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "p7AO6C9-8TkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConvNormPool"
      ],
      "metadata": {
        "id": "e8spFZLq7ccJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNormPool(pl.LightningModule):\n",
        "    \"\"\"Conv Skip-connection module\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hidden_size,\n",
        "        kernel_size,\n",
        "        norm_type='bachnorm'\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv_1 = nn.Conv1d(\n",
        "            in_channels=input_size,\n",
        "            out_channels=hidden_size,\n",
        "            kernel_size=kernel_size\n",
        "        )\n",
        "        self.conv_2 = nn.Conv1d(\n",
        "            in_channels=hidden_size,\n",
        "            out_channels=hidden_size,\n",
        "            kernel_size=kernel_size\n",
        "        )\n",
        "        self.conv_3 = nn.Conv1d(\n",
        "            in_channels=hidden_size,\n",
        "            out_channels=hidden_size,\n",
        "            kernel_size=kernel_size\n",
        "        )\n",
        "        self.swish_1 = Swish()\n",
        "        self.swish_2 = Swish()\n",
        "        self.swish_3 = Swish()\n",
        "        if norm_type == 'group':\n",
        "            self.normalization_1 = nn.GroupNorm(\n",
        "                num_groups=8,\n",
        "                num_channels=hidden_size\n",
        "            )\n",
        "            self.normalization_2 = nn.GroupNorm(\n",
        "                num_groups=8,\n",
        "                num_channels=hidden_size\n",
        "            )\n",
        "            self.normalization_3 = nn.GroupNorm(\n",
        "                num_groups=8,\n",
        "                num_channels=hidden_size\n",
        "            )\n",
        "        else:\n",
        "            self.normalization_1 = nn.BatchNorm1d(num_features=hidden_size)\n",
        "            self.normalization_2 = nn.BatchNorm1d(num_features=hidden_size)\n",
        "            self.normalization_3 = nn.BatchNorm1d(num_features=hidden_size)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        conv1 = self.conv_1(input)\n",
        "        x = self.normalization_1(conv1)\n",
        "        x = self.swish_1(x)\n",
        "        x = F.pad(x, pad=(self.kernel_size - 1, 0))\n",
        "\n",
        "        x = self.conv_2(x)\n",
        "        x = self.normalization_2(x)\n",
        "        x = self.swish_2(x)\n",
        "        x = F.pad(x, pad=(self.kernel_size - 1, 0))\n",
        "\n",
        "        conv3 = self.conv_3(x)\n",
        "        x = self.normalization_3(conv1+conv3)\n",
        "        x = self.swish_3(x)\n",
        "        x = F.pad(x, pad=(self.kernel_size - 1, 0))\n",
        "\n",
        "        x = self.pool(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NPtpv9_j6Ywv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "M7itc5Nq7fCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size = 1,\n",
        "        hid_size = 256,\n",
        "        kernel_size = 5,\n",
        "        num_classes = 5,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = ConvNormPool(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.conv2 = ConvNormPool(\n",
        "            input_size=hid_size,\n",
        "            hidden_size=hid_size//2,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.conv3 = ConvNormPool(\n",
        "            input_size=hid_size//2,\n",
        "            hidden_size=hid_size//4,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d((1))\n",
        "        self.fc = nn.Linear(in_features=hid_size//4, out_features=num_classes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.avgpool(x)\n",
        "        # print(x.shape) # num_features * num_channels\n",
        "        x = x.view(-1, x.size(1) * x.size(2))\n",
        "        x = F.softmax(self.fc(x), dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_HLYf3RW6hNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "BjoJ234gFYOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(pl.LightningModule):\n",
        "    \"\"\"RNN module(cell type lstm or gru)\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hid_size,\n",
        "        num_rnn_layers=1,\n",
        "        dropout_p = 0.2,\n",
        "        bidirectional = False,\n",
        "        rnn_type = 'lstm',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if rnn_type == 'lstm':\n",
        "            self.rnn_layer = nn.LSTM(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hid_size,\n",
        "                num_layers=num_rnn_layers,\n",
        "                dropout=dropout_p if num_rnn_layers>1 else 0,\n",
        "                bidirectional=bidirectional,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.rnn_layer = nn.GRU(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hid_size,\n",
        "                num_layers=num_rnn_layers,\n",
        "                dropout=dropout_p if num_rnn_layers>1 else 0,\n",
        "                bidirectional=bidirectional,\n",
        "                batch_first=True,\n",
        "            )\n",
        "    def forward(self, input):\n",
        "        outputs, hidden_states = self.rnn_layer(input)\n",
        "        return outputs, hidden_states\n"
      ],
      "metadata": {
        "id": "OHzkyOyw74yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Model"
      ],
      "metadata": {
        "id": "f2fFMO5j7ijR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hid_size,\n",
        "        rnn_type,\n",
        "        bidirectional,\n",
        "        n_classes=5,\n",
        "        kernel_size=5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_layer = RNN(\n",
        "            input_size=46,#hid_size * 2 if bidirectional else hid_size,\n",
        "            hid_size=hid_size,\n",
        "            rnn_type=rnn_type,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        self.conv1 = ConvNormPool(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.conv2 = ConvNormPool(\n",
        "            input_size=hid_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d((1))\n",
        "        self.fc = nn.Linear(in_features=hid_size, out_features=n_classes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.conv2(x)\n",
        "        x, _ = self.rnn_layer(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(-1, x.size(1) * x.size(2))\n",
        "        x = F.sigmoid(self.fc(x), dim=1)#.squeeze(1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "bbaUcwi-6j6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Attention Model"
      ],
      "metadata": {
        "id": "dXKYZVjn9Zrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.classification import BinaryAccuracy, BinaryF1Score, BinaryAUROC\n",
        "\n",
        "class RNNAttentionModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hid_size,\n",
        "        rnn_type,\n",
        "        bidirectional,\n",
        "        kernel_size=5,\n",
        "        lr=1e-3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.conv1 = ConvNormPool(\n",
        "            input_size=input_size,  # input_size = 12 for ECG\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.conv2 = ConvNormPool(\n",
        "            input_size=hid_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "\n",
        "        self.rnn_layer = RNN(\n",
        "            input_size=hid_size,\n",
        "            hid_size=hid_size,\n",
        "            rnn_type=rnn_type,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        self.attn = nn.Linear(hid_size, hid_size, bias=False)\n",
        "        self.fc = nn.Linear(in_features=hid_size, out_features=1)  # Binary output\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.lr = lr\n",
        "\n",
        "        # Metrics\n",
        "        self.train_acc = BinaryAccuracy()\n",
        "        self.train_f1 = BinaryF1Score()\n",
        "        self.train_auc = BinaryAUROC()\n",
        "\n",
        "        self.val_acc = BinaryAccuracy()\n",
        "        self.val_f1 = BinaryF1Score()\n",
        "        self.val_auc = BinaryAUROC()\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.permute(0, 2, 1)  # (batch, 12, 1000)\n",
        "        x = self.conv1(input)\n",
        "        x = self.conv2(x)\n",
        "        x = x.permute(0, 2, 1)  # (batch, time_steps, features)\n",
        "\n",
        "        x_out, _ = self.rnn_layer(x)  # (batch, time, hid_size)\n",
        "\n",
        "        attn_weights = torch.softmax(self.attn(x_out), dim=1)  # (batch, time, hid_size)\n",
        "        x = torch.sum(attn_weights * x_out, dim=1)  # (batch, hid_size)\n",
        "\n",
        "        logits = self.fc(x)  # (batch, 1)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x).squeeze()\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "\n",
        "        acc = self.train_acc(probs, y.int())\n",
        "        f1 = self.train_f1(probs, y.int())\n",
        "        auc = self.train_auc(probs, y.int())\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "        self.log(\"train_f1\", f1, prog_bar=True)\n",
        "        self.log(\"train_auc\", auc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x).squeeze()\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "\n",
        "        acc = self.val_acc(probs, y.int())\n",
        "        f1 = self.val_f1(probs, y.int())\n",
        "        auc = self.val_auc(probs, y.int())\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        self.log(\"val_f1\", f1, prog_bar=True)\n",
        "        self.log(\"val_auc\", auc, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n"
      ],
      "metadata": {
        "id": "-1pz8ZdK9d-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Meter:\n",
        "    def __init__(self, n_classes=5):\n",
        "        self.metrics = {}\n",
        "        self.confusion = torch.zeros((n_classes, n_classes))\n",
        "\n",
        "    def update(self, x, y, loss):\n",
        "        x = np.argmax(x.detach().cpu().numpy(), axis=1)\n",
        "        y = y.detach().cpu().numpy()\n",
        "        self.metrics['loss'] += loss\n",
        "        self.metrics['accuracy'] += accuracy_score(x,y)\n",
        "        self.metrics['f1'] += f1_score(x,y,average='macro')\n",
        "        self.metrics['precision'] += precision_score(x, y, average='macro', zero_division=1)\n",
        "        self.metrics['recall'] += recall_score(x,y, average='macro', zero_division=1)\n",
        "\n",
        "        self._compute_cm(x, y)\n",
        "\n",
        "    def _compute_cm(self, x, y):\n",
        "        for prob, target in zip(x, y):\n",
        "            if prob == target:\n",
        "                self.confusion[target][target] += 1\n",
        "            else:\n",
        "                self.confusion[target][prob] += 1\n",
        "\n",
        "    def init_metrics(self):\n",
        "        self.metrics['loss'] = 0\n",
        "        self.metrics['accuracy'] = 0\n",
        "        self.metrics['f1'] = 0\n",
        "        self.metrics['precision'] = 0\n",
        "        self.metrics['recall'] = 0\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return self.metrics\n",
        "\n",
        "    def get_confusion_matrix(self):\n",
        "        return self.confusion\n"
      ],
      "metadata": {
        "id": "uEx3G2D-6rL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Safe collate"
      ],
      "metadata": {
        "id": "m7ks-R4iAfLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def safe_collate(batch):\n",
        "    # Filter out None entries\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None  # Skip entire batch if empty (optional, or raise Exception)\n",
        "\n",
        "    signals, labels = zip(*batch)\n",
        "    signals = pad_sequence(signals, batch_first=True)  # if variable-length ECG\n",
        "    labels = torch.tensor(labels)\n",
        "    return signals, labels\n"
      ],
      "metadata": {
        "id": "ZavmR-kZBZlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXSyMCeb8kyz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "class ECGDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, memmap, train_df, val_df, test_df, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.memmap = memmap\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.test_df = test_df\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = ECGDataset(self.memmap, self.train_df)\n",
        "        self.val_dataset = ECGDataset(self.memmap, self.val_df)\n",
        "        self.test_dataset = ECGDataset(self.memmap, self.test_df)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=11, collate_fn=safe_collate, pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size,  num_workers=11, collate_fn=safe_collate, pin_memory=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=11, collate_fn=safe_collate, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyK98BI28ky0"
      },
      "source": [
        "# Simple LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.classification import BinaryF1Score, BinaryAUROC\n",
        "\n",
        "\n",
        "class LSTMSleepClassifier(pl.LightningModule):\n",
        "    def __init__(self, input_size=12, hidden_size=64, num_layers=2, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "\n",
        "        self.train_f1 = BinaryF1Score()\n",
        "        self.val_f1 = BinaryF1Score()\n",
        "        self.test_f1 = BinaryF1Score()\n",
        "\n",
        "        self.train_auc = BinaryAUROC()\n",
        "        self.val_auc = BinaryAUROC()\n",
        "        self.test_auc = BinaryAUROC()\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * 2, 1)  # bidirectional\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(self.device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C) → needs to be (B, T, 12)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]  # take last timestep\n",
        "        logits = self.fc(out)\n",
        "        return logits.squeeze()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.train()\n",
        "        x, y = batch\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "        auc = self.train_auc(probs, y.int())\n",
        "        acc = (preds == y).float().mean()\n",
        "        f1 = self.train_f1(preds, y)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "        self.log(\"train_f1\", f1, prog_bar=True)\n",
        "        self.log(\"train_auc\", auc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "        auc = self.train_auc(probs, y.int())\n",
        "        acc = (preds == y).float().mean()\n",
        "        f1 = self.val_f1(preds, y)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        self.log(\"val_f1\", f1, prog_bar=True)\n",
        "        self.log(\"val_auc\", auc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "        auc = self.train_auc(probs, y.int())\n",
        "\n",
        "        acc = (preds == y).float().mean()\n",
        "        f1 = self.test_f1(preds, y)\n",
        "\n",
        "        self.log(\"test_loss\", loss, prog_bar=True)\n",
        "        self.log(\"test_acc\", acc, prog_bar=True)\n",
        "        self.log(\"test_f1\", f1, prog_bar=True)\n",
        "        self.log(\"test_auc\", auc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # def configure_gradient_clipping(\n",
        "    #     self,\n",
        "    #     optimizer=None,\n",
        "    #     optimizer_idx=None,\n",
        "    #     gradient_clip_val=None,\n",
        "    #     gradient_clip_algorithm=None\n",
        "    # ):\n",
        "    #     if optimizer is not None:\n",
        "    #         torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
        "\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)  # Reduced lr\n"
      ],
      "metadata": {
        "id": "C6j_HQhvobr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sampling"
      ],
      "metadata": {
        "id": "mZrIbV31ciXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define paths\n",
        "df_path = \"src/data/df_memmap.csv\"\n",
        "train_df_path = \"src/data/train_df.csv\"\n",
        "val_df_path = \"src/data/val_df.csv\"\n",
        "test_df_path = \"src/data/test_df.csv\"\n",
        "\n",
        "# Check if all files exist\n",
        "if os.path.exists(df_path) and os.path.exists(train_df_path) and os.path.exists(val_df_path):\n",
        "  print(\"Skipping Data labeling\")\n",
        "  df = pd.read_csv(df_path)\n",
        "  train_df = pd.read_csv(train_df_path)\n",
        "  val_df = pd.read_csv(val_df_path)\n",
        "  test_df = pd.read_csv(test_df_path)\n",
        "\n",
        "else:\n",
        "  memmap_data = np.memmap(memmap_path, dtype=np.float32, mode='r')\n",
        "  # memmap_data = torch.tensor(memmap_data).to(\"cuda\")\n",
        "  meta = np.load(memmap_meta_path, allow_pickle=True)\n",
        "  start = meta['start']\n",
        "  length = meta['length']\n",
        "\n",
        "  # Load labels CSV\n",
        "  df = df_labels.copy()\n",
        "\n",
        "  # Sanity check\n",
        "  assert len(df) == len(start), \"Mismatch between label and memmap metadata length\"\n",
        "\n",
        "  # Add metadata into DataFrame\n",
        "  df['start'] = start\n",
        "  df['length'] = length\n",
        "\n",
        "  # Now you can split the DataFrame while keeping track of ECG data pointers\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split test set with preserved stroke ratio\n",
        "  train_val_df, test_df = train_test_split(\n",
        "      df, test_size=0.10, stratify=df['Stroke_YN'], random_state=42\n",
        "  )\n",
        "\n",
        "  # Then split stroke/non-stroke from train_val_df as discussed before\n",
        "  stroke_df = train_val_df[train_val_df['Stroke_YN'] == 1]\n",
        "  nonstroke_df = train_val_df[train_val_df['Stroke_YN'] == 0]\n",
        "\n",
        "  # Balanced sampling\n",
        "  train_stroke, val_stroke = train_test_split(stroke_df, test_size=0.1, random_state=42)\n",
        "  train_nonstroke = nonstroke_df.sample(n=len(train_stroke)*2, random_state=42)\n",
        "  val_nonstroke = nonstroke_df.drop(train_nonstroke.index).sample(n=len(val_stroke)*2, random_state=42)\n",
        "\n",
        "  # Final splits\n",
        "  train_df = pd.concat([train_stroke, train_nonstroke]).reset_index(drop=True)\n",
        "  val_df = pd.concat([val_stroke, val_nonstroke]).reset_index(drop=True)\n",
        "  test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "  df.to_csv(\"src/data/df_memmap.csv\", index=False)\n",
        "  train_df.to_csv(\"src/data/train_df.csv\", index=False)\n",
        "  val_df.to_csv(\"src/data/val_df.csv\", index=False)\n",
        "  test_df.to_csv(\"src/data/test_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "y5ip54F3cEWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "labels = train_df[\"Stroke_YN\"].values\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=labels)\n",
        "\n",
        "# class_weights[1] is the weight for positive class\n",
        "pos_weight = class_weights[1] / class_weights[0]  # Convert to ratio\n"
      ],
      "metadata": {
        "id": "0gg6Ndj7Gpv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model and data Initialization"
      ],
      "metadata": {
        "id": "UTqbdVzwYHao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data module\n",
        "ecg_dm = ECGDataModule(\n",
        "    memmap=memmap_data,\n",
        "    train_df=train_df,\n",
        "    val_df=val_df,\n",
        "    test_df=test_df,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "id": "7MBqEkPlvjgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Mlflow for model baseline tracking"
      ],
      "metadata": {
        "id": "NXJk0klE85pI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAqMr6bZ8ky0"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7bJCm-e8ky1"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import MLFlowLogger\n",
        "import mlflow\n",
        "\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import os\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"Zfeng0207\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"af7c8365aec4d3ff7a40563a35ec94d4bc9b4512\"\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"stroke-prediction-dagshub-repo\"\n",
        "# Setup\n",
        "experiment_name = \"lstm-ecg\"\n",
        "tracking_uri = f\"https://dagshub.com/{os.environ['MLFLOW_TRACKING_USERNAME']}/{os.environ['MLFLOW_TRACKING_PROJECTNAME']}.mlflow\"\n",
        "\n",
        "mlflow.set_tracking_uri(tracking_uri)\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"MLflow tracking experiment name: {experiment_name}\")\n",
        "print(f\"Tracking URI: {tracking_uri}\")\n",
        "\n",
        "# Use same URI in logger\n",
        "mlf_logger = MLFlowLogger(\n",
        "    experiment_name=experiment_name,\n",
        "    tracking_uri=tracking_uri,\n",
        "    log_model=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNAttentionModel(12, 64, 'lstm', False)\n",
        "# model = RNNModel(1, 64, 'lstm', True)\n",
        "# model = CNN(num_classes=5, hid_size=128)\n",
        "# model = LSTMSleepClassifier(input_size=12)"
      ],
      "metadata": {
        "id": "p3tiQsti9HmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(logger=mlf_logger, max_epochs=5,gradient_clip_val=1.0)"
      ],
      "metadata": {
        "id": "JocrFrIPsQA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, datamodule=ecg_dm)\n",
        "if mlflow.active_run() is not None:\n",
        "    mlflow.end_run()"
      ],
      "metadata": {
        "id": "b8AwS_tBm2-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics"
      ],
      "metadata": {
        "id": "l3XBvrv0YOqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create lists to hold signals and labels\n",
        "signals = []\n",
        "labels = []\n",
        "\n",
        "for idx in range(len(df)):\n",
        "    start = df.loc[idx, 'start']\n",
        "    length = df.loc[idx, 'length']\n",
        "\n",
        "    # Get and normalize the signal\n",
        "    raw = memmap_data[start : start + length * 12]\n",
        "    normed = (raw - raw.mean()) / (raw.std() + 1e-6)\n",
        "\n",
        "    if np.isnan(normed).any() or np.isinf(normed).any():\n",
        "        continue  # skip bad sample\n",
        "\n",
        "    signal = normed.reshape(length, 12)\n",
        "    signals.append(signal)\n",
        "    labels.append(df.loc[idx, 'Stroke_YN'])\n",
        "\n",
        "# Create DataFrame\n",
        "df_signals = pd.DataFrame({\n",
        "    \"signal\": signals,  # Each row is a 2D numpy array (object dtype)\n",
        "    \"label\": labels\n",
        "})"
      ],
      "metadata": {
        "id": "75Red-Y8Wve3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "bNAbqWrAQKzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqqq tensorflow"
      ],
      "metadata": {
        "id": "-qclOs2jU_Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dropout, Dense\n"
      ],
      "metadata": {
        "id": "iQyNWe6QVunU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df[['start', 'length']].values\n",
        "y_train = train_df['Stroke_YN'].values\n",
        "\n",
        "X_val = val_df[['start', 'length']].values\n",
        "y_val = val_df['Stroke_YN'].values"
      ],
      "metadata": {
        "id": "FFi1tWumWNi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(128, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(SimpleRNN(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:30:47.728276Z",
          "iopub.execute_input": "2024-10-08T11:30:47.728706Z",
          "iopub.status.idle": "2024-10-08T11:30:47.735175Z",
          "shell.execute_reply.started": "2024-10-08T11:30:47.728671Z",
          "shell.execute_reply": "2024-10-08T11:30:47.734054Z"
        },
        "trusted": true,
        "id": "2W4eQ_5uQKzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = build_rnn_model()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:30:49.538397Z",
          "iopub.execute_input": "2024-10-08T11:30:49.538788Z",
          "iopub.status.idle": "2024-10-08T11:30:49.607443Z",
          "shell.execute_reply.started": "2024-10-08T11:30:49.538750Z",
          "shell.execute_reply": "2024-10-08T11:30:49.606453Z"
        },
        "trusted": true,
        "id": "JMjAlHkHQKzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:30:56.322135Z",
          "iopub.execute_input": "2024-10-08T11:30:56.322887Z",
          "iopub.status.idle": "2024-10-08T11:30:56.332313Z",
          "shell.execute_reply.started": "2024-10-08T11:30:56.322849Z",
          "shell.execute_reply": "2024-10-08T11:30:56.331397Z"
        },
        "trusted": true,
        "id": "gl4itdTcQKzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               patience=6,\n",
        "                               restore_best_weights=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.2,\n",
        "                              verbose=1,\n",
        "                              patience=2,\n",
        "                              min_lr=1e-6)\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('model.keras',\n",
        "                                   monitor='val_loss',\n",
        "                                   save_best_only=True)\n",
        "\n",
        "callbacks = [early_stopping, reduce_lr, model_checkpoint]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:30:57.817632Z",
          "iopub.execute_input": "2024-10-08T11:30:57.818353Z",
          "iopub.status.idle": "2024-10-08T11:30:57.824067Z",
          "shell.execute_reply.started": "2024-10-08T11:30:57.818314Z",
          "shell.execute_reply": "2024-10-08T11:30:57.822938Z"
        },
        "trusted": true,
        "id": "SgL4WHb0QKzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_history = rnn_model.fit(X_train_scaled, y_train,\n",
        "                            epochs=10,\n",
        "                            batch_size=32,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            callbacks= callbacks)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:31:03.426039Z",
          "iopub.execute_input": "2024-10-08T11:31:03.426405Z",
          "iopub.status.idle": "2024-10-08T11:43:16.380800Z",
          "shell.execute_reply.started": "2024-10-08T11:31:03.426371Z",
          "shell.execute_reply": "2024-10-08T11:43:16.379913Z"
        },
        "trusted": true,
        "id": "38Ufb-WMQKzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curves_plot(rnn_history, start_epoch=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:16.382770Z",
          "iopub.execute_input": "2024-10-08T11:43:16.383107Z",
          "iopub.status.idle": "2024-10-08T11:43:17.126955Z",
          "shell.execute_reply.started": "2024-10-08T11:43:16.383073Z",
          "shell.execute_reply": "2024-10-08T11:43:17.126068Z"
        },
        "trusted": true,
        "id": "7-maG48oQKzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_test_pred_rnn = rnn_model.predict(X_test_new).argmax(axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:17.128127Z",
          "iopub.execute_input": "2024-10-08T11:43:17.128498Z",
          "iopub.status.idle": "2024-10-08T11:43:19.739828Z",
          "shell.execute_reply.started": "2024-10-08T11:43:17.128458Z",
          "shell.execute_reply": "2024-10-08T11:43:19.738890Z"
        },
        "trusted": true,
        "id": "J2rG2UhuQKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate RNN Model on Test Data\n",
        "rnn_test_loss, rnn_test_acc = rnn_model.evaluate(X_test_new, y_test_new, verbose=0)\n",
        "print(f\"RNN Test Loss: {rnn_test_loss:.4f}\")\n",
        "print(f\"RNN Test Accuracy: {rnn_test_acc:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:19.742149Z",
          "iopub.execute_input": "2024-10-08T11:43:19.742544Z",
          "iopub.status.idle": "2024-10-08T11:43:22.042170Z",
          "shell.execute_reply.started": "2024-10-08T11:43:19.742504Z",
          "shell.execute_reply": "2024-10-08T11:43:22.041135Z"
        },
        "trusted": true,
        "id": "R_I65eZ_QKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report for RNN\n",
        "print(\"RNN Classification Report:\")\n",
        "print(classification_report(y_test_new, y_test_pred_rnn))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:22.043463Z",
          "iopub.execute_input": "2024-10-08T11:43:22.043806Z",
          "iopub.status.idle": "2024-10-08T11:43:22.068563Z",
          "shell.execute_reply.started": "2024-10-08T11:43:22.043771Z",
          "shell.execute_reply": "2024-10-08T11:43:22.067657Z"
        },
        "trusted": true,
        "id": "BExXIXqlQKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [labels[i] for i in range(len(labels))]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:22.069691Z",
          "iopub.execute_input": "2024-10-08T11:43:22.070045Z",
          "iopub.status.idle": "2024-10-08T11:43:22.074841Z",
          "shell.execute_reply.started": "2024-10-08T11:43:22.069996Z",
          "shell.execute_reply": "2024-10-08T11:43:22.073961Z"
        },
        "trusted": true,
        "id": "xtLsxSpjQKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for RNN\n",
        "conf_matrix_rnn = confusion_matrix(y_test_new, y_test_pred_rnn)\n",
        "\n",
        "plot_confusion_matrix(conf_matrix_rnn,\n",
        "                      class_names=class_names,\n",
        "                      show_normed=True,\n",
        "                      figsize=(8,6),\n",
        "                      colorbar=True)\n",
        "\n",
        "plt.title('RNN Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:22.076290Z",
          "iopub.execute_input": "2024-10-08T11:43:22.076652Z",
          "iopub.status.idle": "2024-10-08T11:43:22.490730Z",
          "shell.execute_reply.started": "2024-10-08T11:43:22.076617Z",
          "shell.execute_reply": "2024-10-08T11:43:22.489841Z"
        },
        "trusted": true,
        "id": "ynnueyPvQKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "bSjbH-qPQKzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gru_model():\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GRU(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:22.492191Z",
          "iopub.execute_input": "2024-10-08T11:43:22.492894Z",
          "iopub.status.idle": "2024-10-08T11:43:22.499356Z",
          "shell.execute_reply.started": "2024-10-08T11:43:22.492845Z",
          "shell.execute_reply": "2024-10-08T11:43:22.498480Z"
        },
        "trusted": true,
        "id": "ne3vvLTHQKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = build_gru_model()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:22.500601Z",
          "iopub.execute_input": "2024-10-08T11:43:22.500955Z",
          "iopub.status.idle": "2024-10-08T11:43:22.570750Z",
          "shell.execute_reply.started": "2024-10-08T11:43:22.500912Z",
          "shell.execute_reply": "2024-10-08T11:43:22.569870Z"
        },
        "trusted": true,
        "id": "xq0hNPP1QKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:22.575894Z",
          "iopub.execute_input": "2024-10-08T11:43:22.576208Z",
          "iopub.status.idle": "2024-10-08T11:43:22.584441Z",
          "shell.execute_reply.started": "2024-10-08T11:43:22.576170Z",
          "shell.execute_reply": "2024-10-08T11:43:22.583701Z"
        },
        "trusted": true,
        "id": "CdgkMnyEQKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_history = gru_model.fit(X_train_scaled, y_train,\n",
        "                            epochs=20,\n",
        "                            batch_size=32,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            callbacks= callbacks)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:43:22.585413Z",
          "iopub.execute_input": "2024-10-08T11:43:22.585662Z",
          "iopub.status.idle": "2024-10-08T11:56:10.288152Z",
          "shell.execute_reply.started": "2024-10-08T11:43:22.585633Z",
          "shell.execute_reply": "2024-10-08T11:56:10.287147Z"
        },
        "trusted": true,
        "id": "B13t677dQKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curves_plot(gru_history, start_epoch=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:56:10.289603Z",
          "iopub.execute_input": "2024-10-08T11:56:10.289918Z",
          "iopub.status.idle": "2024-10-08T11:56:11.091154Z",
          "shell.execute_reply.started": "2024-10-08T11:56:10.289885Z",
          "shell.execute_reply": "2024-10-08T11:56:11.090185Z"
        },
        "trusted": true,
        "id": "w5GvtGSRQKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_gru = gru_model.predict(X_test_new).argmax(axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:56:11.092352Z",
          "iopub.execute_input": "2024-10-08T11:56:11.092728Z",
          "iopub.status.idle": "2024-10-08T11:56:13.611718Z",
          "shell.execute_reply.started": "2024-10-08T11:56:11.092691Z",
          "shell.execute_reply": "2024-10-08T11:56:13.610737Z"
        },
        "trusted": true,
        "id": "bCkiF9vfQKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate GRU Model on Test Data\n",
        "gru_test_loss, gru_test_acc = gru_model.evaluate(X_test_new, y_test_new, verbose=0)\n",
        "print(f\"GRU Test Loss: {gru_test_loss:.4f}\")\n",
        "print(f\"GRU Test Accuracy: {gru_test_acc:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:56:13.613249Z",
          "iopub.execute_input": "2024-10-08T11:56:13.613828Z",
          "iopub.status.idle": "2024-10-08T11:56:15.641941Z",
          "shell.execute_reply.started": "2024-10-08T11:56:13.613788Z",
          "shell.execute_reply": "2024-10-08T11:56:15.641011Z"
        },
        "trusted": true,
        "id": "lyxaKYcOQKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU classification report\n",
        "print(\"GRU Classification Report:\")\n",
        "print(classification_report(y_test_new, y_test_pred_gru))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:56:15.643397Z",
          "iopub.execute_input": "2024-10-08T11:56:15.643737Z",
          "iopub.status.idle": "2024-10-08T11:56:15.675818Z",
          "shell.execute_reply.started": "2024-10-08T11:56:15.643700Z",
          "shell.execute_reply": "2024-10-08T11:56:15.674956Z"
        },
        "trusted": true,
        "id": "f48e-YaXQKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix for GRU\n",
        "conf_matrix_gru = confusion_matrix(y_test_new, y_test_pred_gru)\n",
        "\n",
        "plot_confusion_matrix(conf_matrix_gru,\n",
        "                      class_names=class_names,\n",
        "                      show_normed=True,\n",
        "                      figsize=(8,6),\n",
        "                      colorbar=True)\n",
        "\n",
        "plt.title('GRU Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T11:56:15.677118Z",
          "iopub.execute_input": "2024-10-08T11:56:15.677765Z",
          "iopub.status.idle": "2024-10-08T11:56:16.031908Z",
          "shell.execute_reply.started": "2024-10-08T11:56:15.677720Z",
          "shell.execute_reply": "2024-10-08T11:56:16.031049Z"
        },
        "trusted": true,
        "id": "QO0P6WrWQKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "5gPtH4KCQKzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T12:10:25.720908Z",
          "iopub.execute_input": "2024-10-08T12:10:25.721345Z",
          "iopub.status.idle": "2024-10-08T12:10:25.728095Z",
          "shell.execute_reply.started": "2024-10-08T12:10:25.721309Z",
          "shell.execute_reply": "2024-10-08T12:10:25.727057Z"
        },
        "trusted": true,
        "id": "frMCjvfYQKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = build_lstm_model()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T12:15:24.734764Z",
          "iopub.execute_input": "2024-10-08T12:15:24.735174Z",
          "iopub.status.idle": "2024-10-08T12:15:24.806143Z",
          "shell.execute_reply.started": "2024-10-08T12:15:24.735138Z",
          "shell.execute_reply": "2024-10-08T12:15:24.805208Z"
        },
        "trusted": true,
        "id": "S7hNTiYMQKzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T12:15:25.099912Z",
          "iopub.execute_input": "2024-10-08T12:15:25.100274Z",
          "iopub.status.idle": "2024-10-08T12:15:25.108951Z",
          "shell.execute_reply.started": "2024-10-08T12:15:25.100240Z",
          "shell.execute_reply": "2024-10-08T12:15:25.108015Z"
        },
        "trusted": true,
        "id": "jK--VK_3QKzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_history = lstm_model.fit(X_train_scaled, y_train,\n",
        "                            epochs=20,\n",
        "                            batch_size=32,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            callbacks= [reduce_lr, model_checkpoint])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T12:15:42.270883Z",
          "iopub.execute_input": "2024-10-08T12:15:42.271797Z",
          "iopub.status.idle": "2024-10-08T12:28:56.844374Z",
          "shell.execute_reply.started": "2024-10-08T12:15:42.271741Z",
          "shell.execute_reply": "2024-10-08T12:28:56.843377Z"
        },
        "trusted": true,
        "id": "DEk9QubaQKzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curves_plot(lstm_history, start_epoch=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T12:29:12.700080Z",
          "iopub.execute_input": "2024-10-08T12:29:12.700466Z",
          "iopub.status.idle": "2024-10-08T12:29:13.382013Z",
          "shell.execute_reply.started": "2024-10-08T12:29:12.700424Z",
          "shell.execute_reply": "2024-10-08T12:29:13.380919Z"
        },
        "trusted": true,
        "id": "YkXXcK_NQKzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_lstm = lstm_model.predict(X_test_new).argmax(axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T12:29:16.451365Z",
          "iopub.execute_input": "2024-10-08T12:29:16.451787Z",
          "iopub.status.idle": "2024-10-08T12:29:18.887519Z",
          "shell.execute_reply.started": "2024-10-08T12:29:16.451748Z",
          "shell.execute_reply": "2024-10-08T12:29:18.886638Z"
        },
        "trusted": true,
        "id": "SB9oHdZfQKzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate LSTM Model on Test Data\n",
        "lstm_test_loss, lstm_test_acc = lstm_model.evaluate(X_test_new, y_test_new, verbose=0)\n",
        "print(f\"LSTM Test Loss: {lstm_test_loss:.4f}\")\n",
        "print(f\"LSTM Test Accuracy: {lstm_test_acc:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T12:29:20.859897Z",
          "iopub.execute_input": "2024-10-08T12:29:20.860304Z",
          "iopub.status.idle": "2024-10-08T12:29:23.041952Z",
          "shell.execute_reply.started": "2024-10-08T12:29:20.860267Z",
          "shell.execute_reply": "2024-10-08T12:29:23.040930Z"
        },
        "trusted": true,
        "id": "MsgLE5qqQKzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM classification report\n",
        "print(\"LSTM Classification Report:\")\n",
        "print(classification_report(y_test_new, y_test_pred_lstm))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T12:29:23.043729Z",
          "iopub.execute_input": "2024-10-08T12:29:23.044066Z",
          "iopub.status.idle": "2024-10-08T12:29:23.075432Z",
          "shell.execute_reply.started": "2024-10-08T12:29:23.044031Z",
          "shell.execute_reply": "2024-10-08T12:29:23.074519Z"
        },
        "trusted": true,
        "id": "50oYNEb3QKzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix for LSTM\n",
        "conf_matrix_lstm = confusion_matrix(y_test_new, y_test_pred_lstm)\n",
        "\n",
        "plot_confusion_matrix(conf_matrix_lstm,\n",
        "                      class_names=class_names,\n",
        "                      show_normed=True,\n",
        "                      figsize=(8,6),\n",
        "                      colorbar=True)\n",
        "\n",
        "plt.title('LSTM Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T12:29:23.376400Z",
          "iopub.execute_input": "2024-10-08T12:29:23.376775Z",
          "iopub.status.idle": "2024-10-08T12:29:23.806758Z",
          "shell.execute_reply.started": "2024-10-08T12:29:23.376741Z",
          "shell.execute_reply": "2024-10-08T12:29:23.805831Z"
        },
        "trusted": true,
        "id": "1C-F3oylQKzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}