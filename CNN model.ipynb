{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGtcB04peUuQmBJuZFzpNn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zfeng0207/FIT3199-FYP/blob/dev%2Fryuji/CNN%20model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"00_recurrent_stroke_patient.csv\")\n",
        "\n",
        "# Convert categorical target column \"Stroke_Y/N\" to binary (0 or 1)\n",
        "df[\"Stroke_Y/N\"] = df[\"Stroke_Y/N\"].astype(int)\n",
        "\n",
        "# Convert datetime columns to timestamps\n",
        "if \"charttime\" in df.columns:\n",
        "    df[\"charttime\"] = pd.to_datetime(df[\"charttime\"]).astype(int) // 10**9  # Convert to UNIX timestamp\n",
        "\n",
        "# Drop non-numeric columns\n",
        "non_numeric_cols = [\"subject_id\", \"stay_id\", \"icd_code\", \"icd_title\", \"rhythm\", \"gender\", \"anchor_year_group\", \"dod\"]\n",
        "df = df.drop(columns=[col for col in non_numeric_cols if col in df.columns])\n",
        "\n",
        "# Fill missing values (only numeric columns)\n",
        "df = df.apply(pd.to_numeric, errors='coerce')  # Ensure all columns are numeric\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Normalize numerical features\n",
        "features = [col for col in df.columns if col != \"Stroke_Y/N\"]\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# Define input features (X) and target (y)\n",
        "X = df[features].values  # Ensure it's a NumPy array\n",
        "y = df[\"Stroke_Y/N\"].values  # Target variable\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# PyTorch DataLoader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "print(\"Data processing completed successfully!\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define CNN Model for Stroke Prediction\n",
        "class StrokeCNN(nn.Module):\n",
        "    def __init__(self, input_size, num_filters=64, kernel_size=3, dropout=0.3):\n",
        "        super(StrokeCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters * 2, kernel_size=kernel_size, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Calculate output size after convolutions and pooling\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.rand(1, 1, input_size)  # Batch=1, Channels=1, Features=input_size\n",
        "            sample_output = self.pool(torch.relu(self.conv1(sample_input)))\n",
        "            sample_output = self.pool(torch.relu(self.conv2(sample_output)))\n",
        "            self.flattened_size = sample_output.numel()  # Get the total number of features\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Reshape to (batch, 1, features)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(x.shape[0], -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_tensor.shape[1]  # Number of features\n",
        "model = StrokeCNN(input_size)\n",
        "\n",
        "# Loss function & optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Training function with validation accuracy\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=30):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct_train, total_train = 0, 0, 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute training accuracy\n",
        "            predicted = (y_pred > 0.5).float()\n",
        "            correct_train += (predicted == y_batch).sum().item()\n",
        "            total_train += y_batch.size(0)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss, correct_val, total_val = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_val, y_val in val_loader:\n",
        "                y_val_pred = model(X_val)\n",
        "                val_loss += criterion(y_val_pred, y_val).item()\n",
        "                predicted_val = (y_val_pred > 0.5).float()\n",
        "                correct_val += (predicted_val == y_val).sum().item()\n",
        "                total_val += y_val.size(0)\n",
        "\n",
        "        # Compute accuracies\n",
        "        train_acc = correct_train / total_train\n",
        "        val_acc = correct_val / total_val\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss/len(train_loader):.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.4f}, Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
        "              f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=30)\n",
        "\n",
        "# Evaluation function with test accuracy\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_test, y_test in test_loader:\n",
        "            y_test_pred = model(X_test)\n",
        "            predicted = (y_test_pred > 0.5).float()\n",
        "            correct += (predicted == y_test).sum().item()\n",
        "            total += y_test.size(0)\n",
        "    print(f'Test Accuracy: {correct / total:.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUquu2ZVwx7x",
        "outputId": "e91ee597-dee4-4eb4-f082-60d645e622ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data processing completed successfully!\n",
            "Epoch 1/30, Train Loss: 0.3267, Train Acc: 0.8649, Val Loss: 0.3085, Val Acc: 0.8565\n",
            "Epoch 2/30, Train Loss: 0.2885, Train Acc: 0.8649, Val Loss: 0.2948, Val Acc: 0.8565\n",
            "Epoch 3/30, Train Loss: 0.2747, Train Acc: 0.8713, Val Loss: 0.2841, Val Acc: 0.8663\n",
            "Epoch 4/30, Train Loss: 0.2668, Train Acc: 0.8767, Val Loss: 0.2784, Val Acc: 0.8689\n",
            "Epoch 5/30, Train Loss: 0.2627, Train Acc: 0.8774, Val Loss: 0.2716, Val Acc: 0.8703\n",
            "Epoch 6/30, Train Loss: 0.2612, Train Acc: 0.8780, Val Loss: 0.2744, Val Acc: 0.8703\n",
            "Epoch 7/30, Train Loss: 0.2595, Train Acc: 0.8784, Val Loss: 0.2858, Val Acc: 0.8607\n",
            "Epoch 8/30, Train Loss: 0.2571, Train Acc: 0.8785, Val Loss: 0.2679, Val Acc: 0.8703\n",
            "Epoch 9/30, Train Loss: 0.2562, Train Acc: 0.8786, Val Loss: 0.2681, Val Acc: 0.8703\n",
            "Epoch 10/30, Train Loss: 0.2542, Train Acc: 0.8800, Val Loss: 0.2665, Val Acc: 0.8714\n",
            "Epoch 11/30, Train Loss: 0.2528, Train Acc: 0.8796, Val Loss: 0.2621, Val Acc: 0.8707\n",
            "Epoch 12/30, Train Loss: 0.2508, Train Acc: 0.8798, Val Loss: 0.2657, Val Acc: 0.8707\n",
            "Epoch 13/30, Train Loss: 0.2494, Train Acc: 0.8801, Val Loss: 0.2605, Val Acc: 0.8720\n",
            "Epoch 14/30, Train Loss: 0.2484, Train Acc: 0.8803, Val Loss: 0.2649, Val Acc: 0.8707\n",
            "Epoch 15/30, Train Loss: 0.2474, Train Acc: 0.8801, Val Loss: 0.2578, Val Acc: 0.8726\n",
            "Epoch 16/30, Train Loss: 0.2457, Train Acc: 0.8805, Val Loss: 0.2692, Val Acc: 0.8706\n",
            "Epoch 17/30, Train Loss: 0.2440, Train Acc: 0.8812, Val Loss: 0.2672, Val Acc: 0.8716\n",
            "Epoch 18/30, Train Loss: 0.2430, Train Acc: 0.8810, Val Loss: 0.2544, Val Acc: 0.8714\n",
            "Epoch 19/30, Train Loss: 0.2403, Train Acc: 0.8809, Val Loss: 0.2508, Val Acc: 0.8725\n",
            "Epoch 20/30, Train Loss: 0.2381, Train Acc: 0.8834, Val Loss: 0.2551, Val Acc: 0.8718\n",
            "Epoch 21/30, Train Loss: 0.2336, Train Acc: 0.8861, Val Loss: 0.2393, Val Acc: 0.8829\n",
            "Epoch 22/30, Train Loss: 0.2256, Train Acc: 0.8911, Val Loss: 0.2264, Val Acc: 0.8842\n",
            "Epoch 23/30, Train Loss: 0.2118, Train Acc: 0.9011, Val Loss: 0.2164, Val Acc: 0.8818\n",
            "Epoch 24/30, Train Loss: 0.1902, Train Acc: 0.9116, Val Loss: 0.2053, Val Acc: 0.8851\n",
            "Epoch 25/30, Train Loss: 0.1730, Train Acc: 0.9191, Val Loss: 0.1854, Val Acc: 0.9031\n",
            "Epoch 26/30, Train Loss: 0.1642, Train Acc: 0.9237, Val Loss: 0.1763, Val Acc: 0.9273\n",
            "Epoch 27/30, Train Loss: 0.1587, Train Acc: 0.9274, Val Loss: 0.1666, Val Acc: 0.9238\n",
            "Epoch 28/30, Train Loss: 0.1533, Train Acc: 0.9305, Val Loss: 0.1906, Val Acc: 0.9035\n",
            "Epoch 29/30, Train Loss: 0.1530, Train Acc: 0.9300, Val Loss: 0.1564, Val Acc: 0.9330\n",
            "Epoch 30/30, Train Loss: 0.1454, Train Acc: 0.9346, Val Loss: 0.1447, Val Acc: 0.9401\n",
            "Test Accuracy: 0.9434\n"
          ]
        }
      ]
    }
  ]
}