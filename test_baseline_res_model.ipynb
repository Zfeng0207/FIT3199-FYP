{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zfeng0207/FIT3199-FYP/blob/dev%2Fzfeng/test_baseline_res_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting the Notebook\n",
        "\n"
      ],
      "metadata": {
        "id": "XE77-1DFUpTz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhFE7r-5ThiE"
      },
      "source": [
        "## Loading Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0CUDImu9MEW",
        "outputId": "baf3fe00-dc81-435b-e1b8-dd831088953b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GfQLwvb-9N2C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/ECG-MIMIC-main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K_PGqhC8-C_A"
      },
      "outputs": [],
      "source": [
        "!pip install -qqqq mlflow torchmetrics pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vAG6zZC2VOKT"
      },
      "outputs": [],
      "source": [
        "import mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lgFOPav48kyv"
      },
      "outputs": [],
      "source": [
        "memmap_meta_path = \"src/data/memmap/memmap_meta.npz\"\n",
        "memmap_path = \"src/data/memmap/memmap.npy\"\n",
        "df_diag_path = \"src/data/records_w_diag_icd10.csv\"\n",
        "df_memmap_pkl_path = \"src/data/memmap/df_memmap.pkl\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge dataset with labels and ecg paths"
      ],
      "metadata": {
        "id": "X2qs7_d19187"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BgqmAcyf8kyv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_diag = pd.read_csv(df_diag_path)\n",
        "df_mapped = pd.read_pickle(df_memmap_pkl_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multihot_encode(diagnoses, icd_codes):\n",
        "    num_classes = len(icd_codes)\n",
        "    res = np.zeros(num_classes, dtype=np.float32)\n",
        "    for diag in diagnoses:\n",
        "        for i, code in enumerate(icd_codes):  # Iterate through icd_codes with index\n",
        "            if diag.startswith(code):\n",
        "                res[i] = 1\n",
        "                break\n",
        "    return res"
      ],
      "metadata": {
        "id": "4cugp1DhWv2g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "memmap_meta_path = \"/content/drive/MyDrive/Colab Notebooks/ECG-MIMIC-main/src/data/memmap/memmap_meta.npz\"\n",
        "memmap_meta = np.load(memmap_meta_path, allow_pickle=True)\n",
        "df_full = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ECG-MIMIC-main/src/preprocessed_data/records_w_stroke_labels.csv\")\n",
        "df_full[\"start\"] = memmap_meta[\"start\"]\n",
        "df_full[\"length\"] = memmap_meta[\"length\"]\n",
        "\n",
        "df_labels =  df_full[[\"filename\",\n",
        "            \"study_id\",\n",
        "            \"patient_id\",\n",
        "            \"ecg_time\",\n",
        "            \"label_train\",\n",
        "            \"all_diag_all\",\n",
        "            \"label_stroke\",\n",
        "            \"start\",\n",
        "            \"length\"]]\n",
        "\n",
        "target_icd_codes = (\n",
        "    \"I20\", \"I21\", \"I24\", \"I25\",\n",
        "    \"I42\", \"E87\", \"I48\", \"I44\", \"I45\", \"E11\", \"J44\", \"J45\"\n",
        ")\n",
        "\n",
        "df_labels['res'] = df_labels['label_train'].apply(lambda diagnoses: multihot_encode(diagnoses, target_icd_codes))\n",
        "\n",
        "df_labels['stroke_yn'] = df_labels['res'].apply(lambda x: 1 if 1 in x else 0)\n",
        "\n",
        "df_rm_nan = df_labels[df_labels['all_diag_all'].apply(lambda x: len(ast.literal_eval(x)) > 0)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAZEylGAXUTH",
        "outputId": "5d2d07d1-925f-4fdd-8239-fc9c0ea27920"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-c36d8ddba660>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_labels['res'] = df_labels['label_train'].apply(lambda diagnoses: multihot_encode(diagnoses, target_icd_codes))\n",
            "<ipython-input-38-c36d8ddba660>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_labels['stroke_yn'] = df_labels['res'].apply(lambda x: 1 if 1 in x else 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzfifZa58kyz"
      },
      "source": [
        "# ECG Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, memmap, X, y):\n",
        "        self.df = X.reset_index(drop=True)\n",
        "        self.memmap = memmap\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # Access data directly from the DataFrame\n",
        "      start = self.df.loc[idx, 'start']\n",
        "      length = self.df.loc[idx, 'length']\n",
        "      # file_idx = self.df.loc[idx, 'file_idx'] # You might not need file_idx here anymore\n",
        "\n",
        "      # Extract the flat signal slice\n",
        "      signal = self.memmap[start : start + length * 12]  # 12 features per timestep\n",
        "      signal = (signal - signal.mean(axis=0)) / (signal.std(axis=0) + 1e-6)\n",
        "\n",
        "      # Reshape to [length, 12]\n",
        "      signal = signal.reshape(length, 12)\n",
        "\n",
        "      # Convert signal to PyTorch tensor before checking for NaN/inf\n",
        "      signal = torch.tensor(signal, dtype=torch.float32)\n",
        "\n",
        "      if torch.isnan(signal).any() or torch.isinf(signal).any():\n",
        "        return None\n",
        "\n",
        "      label = torch.tensor(self.y.iloc[idx]['res'], dtype=torch.float32)  # Load 'res' as float32\n",
        "      return signal, torch.tensor(label, dtype=torch.long) # signal is already a tensor\n"
      ],
      "metadata": {
        "id": "_xIO7FzUX3tm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "TXSyMCeb8kyz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "class ECGDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, memmap, X_train, y_train, X_val, y_val, X_test, y_test, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.memmap = memmap\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = ECGDataset(self.memmap, self.X_train, self.y_train)\n",
        "        self.val_dataset = ECGDataset(self.memmap, self.X_val, self.y_val)\n",
        "        self.test_dataset = ECGDataset(self.memmap, self.X_test, self.y_test)\n",
        "\n",
        "    def setup_fold_data(self, train_idx, val_idx):\n",
        "        \"\"\"\n",
        "        Prepares data for the current fold using the provided indices.\n",
        "        \"\"\"\n",
        "        # Assuming X_train, y_train, etc. are numpy arrays\n",
        "        self.X_train_fold = self.X_train[train_idx]\n",
        "        self.y_train_fold = self.y_train[train_idx]\n",
        "        self.X_val_fold = self.X_train[val_idx]  # Using a portion of train data for validation\n",
        "        self.y_val_fold = self.y_train[val_idx]\n",
        "\n",
        "        # Update datasets with fold data\n",
        "        self.train_dataset = ECGDataset(self.memmap, self.X_train_fold, self.y_train_fold)\n",
        "        self.val_dataset = ECGDataset(self.memmap, self.X_val_fold, self.y_val_fold)\n",
        "        # self.test_dataset remains the same as it's not used in k-fold training\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=11, collate_fn=safe_collate, pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size,  num_workers=11, collate_fn=safe_collate, pin_memory=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=11, collate_fn=safe_collate, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Swish"
      ],
      "metadata": {
        "id": "QPgELME87YIO"
      }
    },
    {
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "\n",
        "class Swish(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "p7AO6C9-8TkZ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConvNormPool"
      ],
      "metadata": {
        "id": "e8spFZLq7ccJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNormPool(pl.LightningModule):\n",
        "    \"\"\"Conv Skip-connection module\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hidden_size,\n",
        "        kernel_size,\n",
        "        norm_type='bachnorm'\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv_1 = nn.Conv1d(\n",
        "            in_channels=input_size,\n",
        "            out_channels=hidden_size,\n",
        "            kernel_size=kernel_size\n",
        "        )\n",
        "        self.conv_2 = nn.Conv1d(\n",
        "            in_channels=hidden_size,\n",
        "            out_channels=hidden_size,\n",
        "            kernel_size=kernel_size\n",
        "        )\n",
        "        self.conv_3 = nn.Conv1d(\n",
        "            in_channels=hidden_size,\n",
        "            out_channels=hidden_size,\n",
        "            kernel_size=kernel_size\n",
        "        )\n",
        "        self.swish_1 = Swish()\n",
        "        self.swish_2 = Swish()\n",
        "        self.swish_3 = Swish()\n",
        "        if norm_type == 'group':\n",
        "            self.normalization_1 = nn.GroupNorm(\n",
        "                num_groups=8,\n",
        "                num_channels=hidden_size\n",
        "            )\n",
        "            self.normalization_2 = nn.GroupNorm(\n",
        "                num_groups=8,\n",
        "                num_channels=hidden_size\n",
        "            )\n",
        "            self.normalization_3 = nn.GroupNorm(\n",
        "                num_groups=8,\n",
        "                num_channels=hidden_size\n",
        "            )\n",
        "        else:\n",
        "            self.normalization_1 = nn.BatchNorm1d(num_features=hidden_size)\n",
        "            self.normalization_2 = nn.BatchNorm1d(num_features=hidden_size)\n",
        "            self.normalization_3 = nn.BatchNorm1d(num_features=hidden_size)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        conv1 = self.conv_1(input)\n",
        "        x = self.normalization_1(conv1)\n",
        "        x = self.swish_1(x)\n",
        "        x = F.pad(x, pad=(self.kernel_size - 1, 0))\n",
        "\n",
        "        x = self.conv_2(x)\n",
        "        x = self.normalization_2(x)\n",
        "        x = self.swish_2(x)\n",
        "        x = F.pad(x, pad=(self.kernel_size - 1, 0))\n",
        "\n",
        "        conv3 = self.conv_3(x)\n",
        "        x = self.normalization_3(conv1+conv3)\n",
        "        x = self.swish_3(x)\n",
        "        x = F.pad(x, pad=(self.kernel_size - 1, 0))\n",
        "\n",
        "        x = self.pool(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NPtpv9_j6Ywv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "M7itc5Nq7fCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size = 1,\n",
        "        hid_size = 256,\n",
        "        kernel_size = 5,\n",
        "        num_classes = 5,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = ConvNormPool(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.conv2 = ConvNormPool(\n",
        "            input_size=hid_size,\n",
        "            hidden_size=hid_size//2,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.conv3 = ConvNormPool(\n",
        "            input_size=hid_size//2,\n",
        "            hidden_size=hid_size//4,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d((1))\n",
        "        self.fc = nn.Linear(in_features=hid_size//4, out_features=num_classes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.avgpool(x)\n",
        "        # print(x.shape) # num_features * num_channels\n",
        "        x = x.view(-1, x.size(1) * x.size(2))\n",
        "        x = F.softmax(self.fc(x), dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_HLYf3RW6hNe"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "BjoJ234gFYOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(pl.LightningModule):\n",
        "    \"\"\"RNN module(cell type lstm or gru)\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hid_size,\n",
        "        num_rnn_layers=1,\n",
        "        dropout_p = 0.2,\n",
        "        bidirectional = False,\n",
        "        rnn_type = 'lstm',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if rnn_type == 'lstm':\n",
        "            self.rnn_layer = nn.LSTM(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hid_size,\n",
        "                num_layers=num_rnn_layers,\n",
        "                dropout=dropout_p if num_rnn_layers>1 else 0,\n",
        "                bidirectional=bidirectional,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.rnn_layer = nn.GRU(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hid_size,\n",
        "                num_layers=num_rnn_layers,\n",
        "                dropout=dropout_p if num_rnn_layers>1 else 0,\n",
        "                bidirectional=bidirectional,\n",
        "                batch_first=True,\n",
        "            )\n",
        "    def forward(self, input):\n",
        "        outputs, hidden_states = self.rnn_layer(input)\n",
        "        return outputs, hidden_states\n"
      ],
      "metadata": {
        "id": "OHzkyOyw74yI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Model"
      ],
      "metadata": {
        "id": "f2fFMO5j7ijR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hid_size,\n",
        "        rnn_type,\n",
        "        bidirectional,\n",
        "        n_classes=5,\n",
        "        kernel_size=5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_layer = RNN(\n",
        "            input_size=46,#hid_size * 2 if bidirectional else hid_size,\n",
        "            hid_size=hid_size,\n",
        "            rnn_type=rnn_type,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        self.conv1 = ConvNormPool(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.conv2 = ConvNormPool(\n",
        "            input_size=hid_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d((1))\n",
        "        self.fc = nn.Linear(in_features=hid_size, out_features=n_classes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.conv2(x)\n",
        "        x, _ = self.rnn_layer(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(-1, x.size(1) * x.size(2))\n",
        "        x = F.sigmoid(self.fc(x), dim=1)#.squeeze(1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "bbaUcwi-6j6A"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Attention Model"
      ],
      "metadata": {
        "id": "dXKYZVjn9Zrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.classification import MultilabelF1Score\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "class RNNAttentionModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hid_size,\n",
        "        rnn_type,\n",
        "        bidirectional,\n",
        "        kernel_size=5,\n",
        "        lr=1e-3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.conv1 = ConvNormPool(\n",
        "            input_size=input_size,  # input_size = 12 for ECG\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.conv2 = ConvNormPool(\n",
        "            input_size=hid_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "\n",
        "        self.rnn_layer = RNN(\n",
        "            input_size=hid_size,\n",
        "            hid_size=hid_size,\n",
        "            rnn_type=rnn_type,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        self.attn = nn.Linear(hid_size, hid_size, bias=False)\n",
        "        self.fc = nn.Linear(in_features=hid_size, out_features=12)  # Binary output\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.lr = lr\n",
        "\n",
        "        self.fc = nn.Linear(in_features=hid_size, out_features=12)  # Changed for multi-label\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()  # Changed for multi-label\n",
        "\n",
        "        # Metrics for multi-label\n",
        "        # Remove the calls to hamming_loss during initialization\n",
        "        # self.train_hamming_loss = hamming_loss  # Store the function itself\n",
        "        # self.val_hamming_loss = hamming_loss  # Store the function itself\n",
        "        self.train_f1 = MultilabelF1Score(num_labels=12)  # Macro F1 score by default\n",
        "        self.val_f1 = MultilabelF1Score(num_labels=12)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.permute(0, 2, 1)  # (batch, 12, 1000)\n",
        "        x = self.conv1(input)\n",
        "        x = self.conv2(input)\n",
        "        x = x.permute(0, 2, 1)  # (batch, time_steps, features)\n",
        "\n",
        "        x_out, _ = self.rnn_layer(x)  # (batch, time, hid_size)\n",
        "\n",
        "        attn_weights = torch.softmax(self.attn(x_out), dim=1)  # (batch, time, hid_size)\n",
        "        x = torch.sum(attn_weights * x_out, dim=1)  # (batch, hid_size)\n",
        "\n",
        "        logits = self.fc(x)  # (batch, 1)\n",
        "        return logits\n",
        "\n",
        "    def on_train_start(self):\n",
        "        # Log model type as a parameter or tag\n",
        "        # mlflow.pytorch.log_model(self, \"model\") # Registers the model\n",
        "        mlflow.log_param(\"model_type\", \"RNNAttentionModel\")  # Log as parameter\n",
        "        mlflow.set_tag(\"model_type\", \"RNNAttentionModel\")\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        x, y = batch\n",
        "        logits = self(x).squeeze()\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "\n",
        "        # Calculate hamming loss within the training step\n",
        "        hamming_loss_val = hamming_loss(y.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "        acc = self.train_acc(probs, y.int())\n",
        "        f1 = self.train_f1(probs, y.int())\n",
        "        auc = self.train_auc(probs, y.int())\n",
        "\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_hamming_loss\", hamming_loss_val, prog_bar=True)\n",
        "        self.log(\"train_f1\", f1, prog_bar=True)\n",
        "\n",
        "        self.log(\"train_auc\", auc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x).squeeze()\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "\n",
        "        acc = self.val_acc(probs, y.int())\n",
        "        f1 = self.val_f1(probs, y.int())\n",
        "        auc = self.val_auc(probs, y.int())\n",
        "        # Calculate hamming loss within the validation step\n",
        "        hamming_loss_val = hamming_loss(y.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "\n",
        "        self.log(\"val_hamming_loss\", hamming_loss_val, prog_bar=True)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        self.log(\"val_f1\", f1, prog_bar=True)\n",
        "        self.log(\"val_auc\", auc, prog_bar=True)\n",
        "\n",
        "    def on_test_start(self):\n",
        "        self.test_probs = []\n",
        "        self.test_preds = []\n",
        "        self.test_targets = []\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x).squeeze()\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs > 0.5).int()\n",
        "\n",
        "        self.test_probs.append(probs.detach().cpu())\n",
        "        self.test_preds.append(preds.detach().cpu())\n",
        "        self.test_targets.append(y.detach().cpu())\n",
        "\n",
        "    def on_test_end(self):\n",
        "        self.all_probs = torch.cat(self.test_probs)\n",
        "        self.all_preds = torch.cat(self.test_preds)\n",
        "        self.all_targets = torch.cat(self.test_targets)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
      ],
      "metadata": {
        "id": "-1pz8ZdK9d-1"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Safe collate"
      ],
      "metadata": {
        "id": "m7ks-R4iAfLt"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def safe_collate(batch):\n",
        "    # Filter out None entries\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None  # Skip entire batch if empty (optional, or raise Exception)\n",
        "\n",
        "    signals, labels = zip(*batch)\n",
        "    signals = pad_sequence(signals, batch_first=True)  # if variable-length ECG\n",
        "    labels = torch.tensor(labels)\n",
        "    return signals, labels\n"
      ],
      "metadata": {
        "id": "ZavmR-kZBZlc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyK98BI28ky0"
      },
      "source": [
        "# Simple LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.classification import BinaryF1Score, BinaryAUROC\n",
        "\n",
        "\n",
        "class LSTMClassifier(pl.LightningModule):\n",
        "    def __init__(self, input_size=12, hidden_size=64, num_layers=2, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.train_f1 = BinaryF1Score()\n",
        "        self.val_f1 = BinaryF1Score()\n",
        "        self.test_f1 = BinaryF1Score()\n",
        "\n",
        "        self.train_auc = BinaryAUROC()\n",
        "        self.val_auc = BinaryAUROC()\n",
        "        self.test_auc = BinaryAUROC()\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * 2, 1)  # bidirectional\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(self.device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C) → needs to be (B, T, 12)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]  # take last timestep\n",
        "        logits = self.fc(out)\n",
        "        return logits.squeeze()\n",
        "\n",
        "    def on_train_start(self):\n",
        "      # Log model type as a parameter or tag\n",
        "      mlflow.pytorch.log_model(self, \"model\") # Registers the model\n",
        "      mlflow.log_param(\"model_type\", \"LSTM\")  # Log as parameter\n",
        "      mlflow.set_tag(\"model_type\", \"LSTM\")\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.train()\n",
        "        x, y = batch\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "        auc = self.train_auc(probs, y.int())\n",
        "        acc = (preds == y).float().mean()\n",
        "        f1 = self.train_f1(preds, y)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "        self.log(\"train_f1\", f1, prog_bar=True)\n",
        "        self.log(\"train_auc\", auc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "        auc = self.train_auc(probs, y.int())\n",
        "        acc = (preds == y).float().mean()\n",
        "        f1 = self.val_f1(preds, y)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        self.log(\"val_f1\", f1, prog_bar=True)\n",
        "        self.log(\"val_auc\", auc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = probs > 0.5\n",
        "        auc = self.train_auc(probs, y.int())\n",
        "\n",
        "        acc = (preds == y).float().mean()\n",
        "        f1 = self.test_f1(preds, y)\n",
        "\n",
        "        self.log(\"test_loss\", loss, prog_bar=True)\n",
        "        self.log(\"test_acc\", acc, prog_bar=True)\n",
        "        self.log(\"test_f1\", f1, prog_bar=True)\n",
        "        self.log(\"test_auc\", auc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # def configure_gradient_clipping(\n",
        "    #     self,\n",
        "    #     optimizer=None,\n",
        "    #     optimizer_idx=None,\n",
        "    #     gradient_clip_val=None,\n",
        "    #     gradient_clip_algorithm=None\n",
        "    # ):\n",
        "    #     if optimizer is not None:\n",
        "    #         torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
        "\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)  # Reduced lr\n"
      ],
      "metadata": {
        "id": "C6j_HQhvobr4"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sampling"
      ],
      "metadata": {
        "id": "mZrIbV31ciXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install scikit-multilearn"
      ],
      "metadata": {
        "id": "UiOXyRF3j4Ce",
        "outputId": "80841676-3d56-4ac0-d595-d7e085ab04e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "oa5nZ63xnnai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "m06icu3bUQiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Define paths\n",
        "# df_path = \"src/data/df_memmap.csv\"\n",
        "# train_df_path = \"src/data/train_df.csv\"\n",
        "# val_df_path = \"src/data/val_df.csv\"\n",
        "# test_df_path = \"src/data/test_df.csv\"\n",
        "\n",
        "#   # Load labels CSV\n",
        "# df = df_labels.copy()\n",
        "\n",
        "# # Now you can split the DataFrame while keeping track of ECG data pointers\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Split test set with preserved stroke ratio\n",
        "# train_val_df, test_df = train_test_split(\n",
        "#     df, test_size=0.10, stratify=df['stroke_yn'], random_state=42\n",
        "# )\n",
        "\n",
        "# # Then split stroke/non-stroke from train_val_df as discussed before\n",
        "# stroke_df = train_val_df[train_val_df['stroke_yn'] == 1]\n",
        "# nonstroke_df = train_val_df[train_val_df['stroke_yn'] == 0]\n",
        "\n",
        "# # Balanced sampling\n",
        "# train_stroke, val_stroke = train_test_split(stroke_df, test_size=0.1, random_state=42)\n",
        "# train_nonstroke = nonstroke_df.sample(n=len(train_stroke)*2, random_state=42)\n",
        "# val_nonstroke = nonstroke_df.drop(train_nonstroke.index).sample(n=len(val_stroke)*2, random_state=42)\n",
        "\n",
        "# # Final splits\n",
        "# train_df = pd.concat([train_stroke, train_nonstroke]).reset_index(drop=True)\n",
        "# val_df = pd.concat([val_stroke, val_nonstroke]).reset_index(drop=True)\n",
        "# test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "# # df.to_csv(\"src/data/df_memmap.csv\", index=False)\n",
        "# # train_df.to_csv(\"src/data/train_df.csv\", index=False)\n",
        "# # val_df.to_csv(\"src/data/val_df.csv\", index=False)\n",
        "# # test_df.to_csv(\"src/data/test_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "y5ip54F3cEWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "labels = y_train\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=labels)\n",
        "\n",
        "# class_weights[1] is the weight for positive class\n",
        "pos_weight = class_weights[1] / class_weights[0]  # Convert to ratio\n"
      ],
      "metadata": {
        "id": "0gg6Ndj7Gpv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model and data Initialization"
      ],
      "metadata": {
        "id": "UTqbdVzwYHao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data module\n",
        "ecg_dm = ECGDataModule(\n",
        "    memmap=memmap_data,\n",
        "    X_train = X_train,\n",
        "    X_val = X_val,\n",
        "    X_test = X_test,\n",
        "    y_train = y_train,\n",
        "    y_val = y_val,\n",
        "    y_test = y_test,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "id": "7MBqEkPlvjgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Mlflow for model baseline tracking"
      ],
      "metadata": {
        "id": "NXJk0klE85pI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7bJCm-e8ky1"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import MLFlowLogger\n",
        "import mlflow\n",
        "\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import os\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"Zfeng0207\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"af7c8365aec4d3ff7a40563a35ec94d4bc9b4512\"\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"stroke-prediction-dagshub-repo\"\n",
        "# Setup\n",
        "experiment_name = \"lstm-ecg\"\n",
        "tracking_uri = f\"https://dagshub.com/{os.environ['MLFLOW_TRACKING_USERNAME']}/{os.environ['MLFLOW_TRACKING_PROJECTNAME']}.mlflow\"\n",
        "\n",
        "mlflow.set_tracking_uri(tracking_uri)\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"MLflow tracking experiment name: {experiment_name}\")\n",
        "print(f\"Tracking URI: {tracking_uri}\")\n",
        "\n",
        "# Use same URI in logger\n",
        "mlf_logger = MLFlowLogger(\n",
        "    experiment_name=experiment_name,\n",
        "    tracking_uri=tracking_uri,\n",
        "    log_model=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection"
      ],
      "metadata": {
        "id": "h-SQHYpewUK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNAttentionModel(12, 64, 'lstm', False)\n",
        "# model = RNNModel(12, 64, 'lstm', True)\n",
        "# model = CNN(num_classes=5, hid_size=128)\n",
        "# model = LSTMClassifier(input_size=12)"
      ],
      "metadata": {
        "id": "p3tiQsti9HmN"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='./checkpoints',\n",
        "    filename='ecgmodel-ep{epoch:02d}-vloss{val_loss:.2f}',\n",
        "    save_top_k=1,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "\n",
        "# Update the Trainer to use the callback\n",
        "trainer = Trainer(\n",
        "    # logger=mlf_logger,\n",
        "    max_epochs=1,\n",
        "    gradient_clip_val=1.0,\n",
        "    # callbacks=[checkpoint_callback]  # Add callback here\n",
        ")"
      ],
      "metadata": {
        "id": "JocrFrIPsQA7",
        "outputId": "b1623ad1-0403-46b0-d325-674f4f628a39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "fTmxQZaPxgoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "import numpy as np\n",
        "\n",
        "df = df_rm_nan\n",
        "X = df.drop(columns='res')  # Features\n",
        "\n",
        "# Convert 'res' column to a 2D numpy array\n",
        "y = np.array(df['res'].tolist())\n",
        "\n",
        "# Now perform the split\n",
        "X_train, y_train, X_test, y_test = iterative_train_test_split(\n",
        "    X.values, y, test_size=0.2\n",
        ")\n",
        "\n",
        "X_train, y_train, X_val, y_val = iterative_train_test_split(\n",
        "    X_train, y_train, test_size=0.1\n",
        ")\n",
        "\n",
        "memmap_data = np.memmap('/content/drive/MyDrive/Colab Notebooks/ECG-MIMIC-main/src/data/memmap/memmap.npy', dtype='float32', mode='r')\n",
        "# --- Create your DataModule here ---\n",
        "ecg_dm = ECGDataModule(memmap=memmap_data,  # Assuming you have memmap_data\n",
        "                        X_train=X_train,\n",
        "                        y_train=y_train,\n",
        "                        X_val=X_val,\n",
        "                        y_val=y_val,\n",
        "                        X_test=X_test,\n",
        "                        y_test=y_test,\n",
        "                        batch_size=64)\n"
      ],
      "metadata": {
        "id": "b8AwS_tBm2-W"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skmultilearn.model_selection import IterativeStratification\n",
        "\n",
        "k_fold = IterativeStratification(n_splits=2, order=1)\n",
        "\n",
        "# Convert X_train to DataFrame if necessary for proper indexing\n",
        "if not isinstance(X_train, pd.DataFrame):\n",
        "    X_train = pd.DataFrame(X_train, index=X.index[np.where(np.isin(X.index, X_train))])\n",
        "\n",
        "\n",
        "for train_idx, val_idx in k_fold.split(X_train, y_train):  # Split based on X_train and y_train\n",
        "    # Prepare data for the current fold\n",
        "    ecg_dm.setup_fold_data(train_idx, val_idx)  # Call the setup_fold_data method to prep data\n",
        "\n",
        "    # Now you can use the indices to select data for the current fold\n",
        "    trainer.fit(model, datamodule=ecg_dm)  # Pass the model and DataModule\n",
        "    result = trainer.predict(model, datamodule=ecg_dm)"
      ],
      "metadata": {
        "id": "HJxqjw5ExuZO",
        "outputId": "443ac7fc-f833-47bf-db89-5c2c30b81964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-13662f013089>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Convert X_train to DataFrame if necessary for proper indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_arraysetops_impl.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \"\"\"\n\u001b[1;32m   1104\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m     return _in1d(element, test_elements, assume_unique=assume_unique,\n\u001b[0m\u001b[1;32m   1106\u001b[0m                  invert=invert, kind=kind).reshape(element.shape)\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_arraysetops_impl.py\u001b[0m in \u001b[0;36m_in1d\u001b[0;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m                 \u001b[0mmask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(model, datamodule=ecg_dm)"
      ],
      "metadata": {
        "id": "wohNId8u1LT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics"
      ],
      "metadata": {
        "id": "l3XBvrv0YOqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "CDqNArFPMyIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc,\n",
        "    accuracy_score, f1_score, roc_auc_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After test finishes\n",
        "y_true = model.all_targets.numpy()\n",
        "y_pred = model.all_preds.numpy()\n",
        "y_prob = model.all_probs.numpy()\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "roc_auc_score_val = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC Score: {roc_auc_score_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "JRn3CfB6M3N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create lists to hold signals and labels\n",
        "signals = []\n",
        "labels = []\n",
        "\n",
        "for idx in range(len(df)):\n",
        "    start = df.loc[idx, 'start']\n",
        "    length = df.loc[idx, 'length']\n",
        "\n",
        "    # Get and normalize the signal\n",
        "    raw = memmap_data[start : start + length * 12]\n",
        "    normed = (raw - raw.mean()) / (raw.std() + 1e-6)\n",
        "\n",
        "    if np.isnan(normed).any() or np.isinf(normed).any():\n",
        "        continue  # skip bad sample\n",
        "\n",
        "    signal = normed.reshape(length, 12)\n",
        "    signals.append(signal)\n",
        "    labels.append(df.loc[idx, 'Stroke_YN'])\n",
        "\n",
        "# Create DataFrame\n",
        "df_signals = pd.DataFrame({\n",
        "    \"signal\": signals,  # Each row is a 2D numpy array (object dtype)\n",
        "    \"label\": labels\n",
        "})"
      ],
      "metadata": {
        "id": "75Red-Y8Wve3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}