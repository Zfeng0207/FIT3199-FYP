{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1mHnehdIpQYRTmu5KayfBQHYVjY0I39Ma",
      "authorship_tag": "ABX9TyNQYUc8Z/rwkY/o6nNiTqoT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zfeng0207/FIT3199-FYP/blob/dev%2Fryuji/FYP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"00_recurrent_stroke_patient.csv\")\n",
        "\n",
        "# Convert categorical target column \"Stroke_Y/N\" to binary (0 or 1)\n",
        "df[\"Stroke_Y/N\"] = df[\"Stroke_Y/N\"].astype(int)\n",
        "\n",
        "# Convert datetime columns to timestamps\n",
        "if \"charttime\" in df.columns:\n",
        "    df[\"charttime\"] = pd.to_datetime(df[\"charttime\"]).astype(int) // 10**9  # Convert to UNIX timestamp\n",
        "\n",
        "# Drop non-numeric columns\n",
        "non_numeric_cols = [\"subject_id\", \"stay_id\", \"icd_code\", \"icd_title\", \"rhythm\", \"gender\", \"anchor_year_group\", \"dod\"]\n",
        "df = df.drop(columns=[col for col in non_numeric_cols if col in df.columns])\n",
        "\n",
        "# Fill missing values (only numeric columns)\n",
        "df = df.apply(pd.to_numeric, errors='coerce')  # Ensure all columns are numeric\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Normalize numerical features\n",
        "features = [col for col in df.columns if col != \"Stroke_Y/N\"]\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# Define input features (X) and target (y)\n",
        "X = df[features].values  # Ensure it's a NumPy array\n",
        "y = df[\"Stroke_Y/N\"].values  # Target variable\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# PyTorch DataLoader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "print(\"Data processing completed successfully!\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define CNN Model for Stroke Prediction\n",
        "class StrokeCNN(nn.Module):\n",
        "    def __init__(self, input_size, num_filters=64, kernel_size=3, dropout=0.3):\n",
        "        super(StrokeCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters * 2, kernel_size=kernel_size, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Calculate output size after convolutions and pooling\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.rand(1, 1, input_size)  # Batch=1, Channels=1, Features=input_size\n",
        "            sample_output = self.pool(torch.relu(self.conv1(sample_input)))\n",
        "            sample_output = self.pool(torch.relu(self.conv2(sample_output)))\n",
        "            self.flattened_size = sample_output.numel()  # Get the total number of features\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Reshape to (batch, 1, features)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(x.shape[0], -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_tensor.shape[1]  # Number of features\n",
        "model = StrokeCNN(input_size)\n",
        "\n",
        "# Loss function & optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Training function with validation accuracy\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=30):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct_train, total_train = 0, 0, 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute training accuracy\n",
        "            predicted = (y_pred > 0.5).float()\n",
        "            correct_train += (predicted == y_batch).sum().item()\n",
        "            total_train += y_batch.size(0)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss, correct_val, total_val = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_val, y_val in val_loader:\n",
        "                y_val_pred = model(X_val)\n",
        "                val_loss += criterion(y_val_pred, y_val).item()\n",
        "                predicted_val = (y_val_pred > 0.5).float()\n",
        "                correct_val += (predicted_val == y_val).sum().item()\n",
        "                total_val += y_val.size(0)\n",
        "\n",
        "        # Compute accuracies\n",
        "        train_acc = correct_train / total_train\n",
        "        val_acc = correct_val / total_val\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss/len(train_loader):.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.4f}, Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
        "              f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=30)\n",
        "\n",
        "# Evaluation function with test accuracy\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_test, y_test in test_loader:\n",
        "            y_test_pred = model(X_test)\n",
        "            predicted = (y_test_pred > 0.5).float()\n",
        "            correct += (predicted == y_test).sum().item()\n",
        "            total += y_test.size(0)\n",
        "    print(f'Test Accuracy: {correct / total:.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUquu2ZVwx7x",
        "outputId": "e91ee597-dee4-4eb4-f082-60d645e622ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data processing completed successfully!\n",
            "Epoch 1/30, Train Loss: 0.3267, Train Acc: 0.8649, Val Loss: 0.3085, Val Acc: 0.8565\n",
            "Epoch 2/30, Train Loss: 0.2885, Train Acc: 0.8649, Val Loss: 0.2948, Val Acc: 0.8565\n",
            "Epoch 3/30, Train Loss: 0.2747, Train Acc: 0.8713, Val Loss: 0.2841, Val Acc: 0.8663\n",
            "Epoch 4/30, Train Loss: 0.2668, Train Acc: 0.8767, Val Loss: 0.2784, Val Acc: 0.8689\n",
            "Epoch 5/30, Train Loss: 0.2627, Train Acc: 0.8774, Val Loss: 0.2716, Val Acc: 0.8703\n",
            "Epoch 6/30, Train Loss: 0.2612, Train Acc: 0.8780, Val Loss: 0.2744, Val Acc: 0.8703\n",
            "Epoch 7/30, Train Loss: 0.2595, Train Acc: 0.8784, Val Loss: 0.2858, Val Acc: 0.8607\n",
            "Epoch 8/30, Train Loss: 0.2571, Train Acc: 0.8785, Val Loss: 0.2679, Val Acc: 0.8703\n",
            "Epoch 9/30, Train Loss: 0.2562, Train Acc: 0.8786, Val Loss: 0.2681, Val Acc: 0.8703\n",
            "Epoch 10/30, Train Loss: 0.2542, Train Acc: 0.8800, Val Loss: 0.2665, Val Acc: 0.8714\n",
            "Epoch 11/30, Train Loss: 0.2528, Train Acc: 0.8796, Val Loss: 0.2621, Val Acc: 0.8707\n",
            "Epoch 12/30, Train Loss: 0.2508, Train Acc: 0.8798, Val Loss: 0.2657, Val Acc: 0.8707\n",
            "Epoch 13/30, Train Loss: 0.2494, Train Acc: 0.8801, Val Loss: 0.2605, Val Acc: 0.8720\n",
            "Epoch 14/30, Train Loss: 0.2484, Train Acc: 0.8803, Val Loss: 0.2649, Val Acc: 0.8707\n",
            "Epoch 15/30, Train Loss: 0.2474, Train Acc: 0.8801, Val Loss: 0.2578, Val Acc: 0.8726\n",
            "Epoch 16/30, Train Loss: 0.2457, Train Acc: 0.8805, Val Loss: 0.2692, Val Acc: 0.8706\n",
            "Epoch 17/30, Train Loss: 0.2440, Train Acc: 0.8812, Val Loss: 0.2672, Val Acc: 0.8716\n",
            "Epoch 18/30, Train Loss: 0.2430, Train Acc: 0.8810, Val Loss: 0.2544, Val Acc: 0.8714\n",
            "Epoch 19/30, Train Loss: 0.2403, Train Acc: 0.8809, Val Loss: 0.2508, Val Acc: 0.8725\n",
            "Epoch 20/30, Train Loss: 0.2381, Train Acc: 0.8834, Val Loss: 0.2551, Val Acc: 0.8718\n",
            "Epoch 21/30, Train Loss: 0.2336, Train Acc: 0.8861, Val Loss: 0.2393, Val Acc: 0.8829\n",
            "Epoch 22/30, Train Loss: 0.2256, Train Acc: 0.8911, Val Loss: 0.2264, Val Acc: 0.8842\n",
            "Epoch 23/30, Train Loss: 0.2118, Train Acc: 0.9011, Val Loss: 0.2164, Val Acc: 0.8818\n",
            "Epoch 24/30, Train Loss: 0.1902, Train Acc: 0.9116, Val Loss: 0.2053, Val Acc: 0.8851\n",
            "Epoch 25/30, Train Loss: 0.1730, Train Acc: 0.9191, Val Loss: 0.1854, Val Acc: 0.9031\n",
            "Epoch 26/30, Train Loss: 0.1642, Train Acc: 0.9237, Val Loss: 0.1763, Val Acc: 0.9273\n",
            "Epoch 27/30, Train Loss: 0.1587, Train Acc: 0.9274, Val Loss: 0.1666, Val Acc: 0.9238\n",
            "Epoch 28/30, Train Loss: 0.1533, Train Acc: 0.9305, Val Loss: 0.1906, Val Acc: 0.9035\n",
            "Epoch 29/30, Train Loss: 0.1530, Train Acc: 0.9300, Val Loss: 0.1564, Val Acc: 0.9330\n",
            "Epoch 30/30, Train Loss: 0.1454, Train Acc: 0.9346, Val Loss: 0.1447, Val Acc: 0.9401\n",
            "Test Accuracy: 0.9434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8U9yjOP0RnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNv2mmsx0RuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score\n",
        "\n",
        "# --------------------- Data Preprocessing ---------------------\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/FIT3199-FYP/ecg_data/records_w_diag_icd10.csv\")\n",
        "df.head()\n",
        "# # Convert categorical target column \"Stroke_Y/N\" to binary (0 or 1)\n",
        "# df[\"Stroke_Y/N\"] = df[\"Stroke_Y/N\"].astype(int)\n",
        "\n",
        "# # Convert 'charttime' to datetime and sort by subject_id, charttime\n",
        "# df['charttime'] = pd.to_datetime(df['charttime'], errors='coerce')\n",
        "# df = df.sort_values(by=['subject_id', 'charttime'])\n",
        "\n",
        "# # Drop non-relevant columns\n",
        "# columns_to_drop = [\"stay_id_x\", \"stay_id_y\", \"charttime\", \"dod\", \"icd_title\"]\n",
        "# df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# # Handle missing values by filling with column mean for numeric columns only\n",
        "# numeric_data = df.select_dtypes(include=np.number)\n",
        "# df[numeric_data.columns] = numeric_data.fillna(numeric_data.mean())\n",
        "\n",
        "# # Ensure all remaining columns are numeric\n",
        "# df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# # Our Stroke Target Column\n",
        "# target_column = \"Stroke_Y/N\"\n",
        "\n",
        "# # --------------------- Time-Series Sequence Creation ---------------------\n",
        "\n",
        "# def create_sequences(df, n_previous=3):\n",
        "#     sequences, labels = [], []\n",
        "\n",
        "#     patient_groups = df.groupby(\"subject_id\")  # Group by patient\n",
        "#     for _, group in patient_groups:\n",
        "#         group = group.drop(columns=[\"subject_id\"])  # Drop ID for training\n",
        "#         if len(group) < n_previous:\n",
        "#             continue  # Skip patients with too few records\n",
        "\n",
        "#         # Ensure only numeric values\n",
        "#         group = group.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "#         X_patient = group.drop(columns=[\"Stroke_Y/N\"]).values\n",
        "#         y_patient = group[\"Stroke_Y/N\"].values\n",
        "\n",
        "#         # Create sequences of length `n_previous`\n",
        "#         for i in range(len(group) - n_previous + 1):\n",
        "#             seq_X = X_patient[i:i + n_previous]  # Past admissions\n",
        "#             seq_y = y_patient[i + n_previous - 1]  # Predict next admission stroke outcome\n",
        "#             sequences.append(seq_X)\n",
        "#             labels.append(seq_y)\n",
        "\n",
        "#     return np.array(sequences, dtype=np.float32), np.array(labels, dtype=np.float32)\n",
        "\n",
        "# # Generate time-series sequences\n",
        "# X_seq, y_seq = create_sequences(df, n_previous=3)\n",
        "\n",
        "# # Replace NaN values with 0\n",
        "# X_seq = np.nan_to_num(X_seq, nan=0.0)\n",
        "# y_seq = np.nan_to_num(y_seq, nan=0.0)\n",
        "\n",
        "# # Convert to PyTorch tensors\n",
        "# X_tensor = torch.tensor(X_seq, dtype=torch.float32)\n",
        "# y_tensor = torch.tensor(y_seq, dtype=torch.float32).unsqueeze(1)  # Shape: (N,1)\n",
        "\n",
        "# # Debugging Output\n",
        "# print(f\"X_tensor shape: {X_tensor.shape}\")  # Should be (samples, time_steps, features)\n",
        "# print(f\"y_tensor shape: {y_tensor.shape}\")  # Should be (samples, 1)\n",
        "\n",
        "# # Check if GPU is available\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# X_tensor, y_tensor = X_tensor.to(device), y_tensor.to(device)\n",
        "\n",
        "# # --------------------- Train-Validation-Test Split ---------------------\n",
        "\n",
        "# train_size = int(0.7 * len(X_tensor))\n",
        "# val_size = int(0.15 * len(X_tensor))\n",
        "# test_size = len(X_tensor) - train_size - val_size\n",
        "\n",
        "# train_data, val_data, test_data = random_split(TensorDataset(X_tensor, y_tensor), [train_size, val_size, test_size])\n",
        "\n",
        "# train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "# val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
        "# test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# # --------------------- Define CNN Model ---------------------\n",
        "\n",
        "# class StrokeCNN(nn.Module):\n",
        "#     def __init__(self, num_features, num_filters=64, kernel_size=2, dropout=0.3):\n",
        "#         super(StrokeCNN, self).__init__()\n",
        "\n",
        "#         self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=num_filters, kernel_size=kernel_size, padding=1)\n",
        "#         self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters * 2, kernel_size=kernel_size, padding=1)\n",
        "\n",
        "#         self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             sample_input = torch.rand(1, num_features, 3)\n",
        "#             sample_output = self.pool(torch.relu(self.conv1(sample_input)))\n",
        "#             sample_output = self.pool(torch.relu(self.conv2(sample_output)))\n",
        "#             self.flattened_size = sample_output.numel()\n",
        "\n",
        "#         self.fc1 = nn.Linear(self.flattened_size, 128)\n",
        "#         self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.permute(0, 2, 1)\n",
        "#         x = self.pool(torch.relu(self.conv1(x)))\n",
        "#         x = self.pool(torch.relu(self.conv2(x)))\n",
        "#         x = x.view(x.shape[0], -1)\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return self.sigmoid(x)\n",
        "\n",
        "# # --------------------- Training ---------------------\n",
        "\n",
        "# num_features = X_seq.shape[2]\n",
        "# model = StrokeCNN(num_features).to(device)\n",
        "# criterion = nn.BCELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# num_epochs = 10\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     train_loss = 0.0\n",
        "#     train_correct = 0\n",
        "#     train_total = 0\n",
        "\n",
        "#     for inputs, targets in train_loader:\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)  # Move to GPU\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         train_loss += loss.item()\n",
        "#         predicted = (outputs >= 0.5).float()\n",
        "#         train_correct += (predicted == targets).sum().item()\n",
        "#         train_total += targets.size(0)\n",
        "\n",
        "#     train_loss /= len(train_loader)\n",
        "#     train_accuracy = train_correct / train_total\n",
        "\n",
        "#     model.eval()\n",
        "#     val_loss = 0.0\n",
        "#     val_correct = 0\n",
        "#     val_total = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, targets in val_loader:\n",
        "#             inputs, targets = inputs.to(device), targets.to(device)\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, targets)\n",
        "#             val_loss += loss.item()\n",
        "#             predicted = (outputs >= 0.5).float()\n",
        "#             val_correct += (predicted == targets).sum().item()\n",
        "#             val_total += targets.size(0)\n",
        "\n",
        "#     val_loss /= len(val_loader)\n",
        "#     val_accuracy = val_correct / val_total\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} - \"\n",
        "#           f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "# # --------------------- Evaluation ---------------------\n",
        "\n",
        "# def evaluate(model, dataloader):\n",
        "#     model.eval()\n",
        "#     y_true, y_pred = [], []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, targets in dataloader:\n",
        "#             inputs, targets = inputs.to(device), targets.to(device)\n",
        "#             outputs = model(inputs).cpu().numpy()\n",
        "#             y_pred.extend(outputs)\n",
        "#             y_true.extend(targets.cpu().numpy())\n",
        "\n",
        "#     y_pred_binary = (np.array(y_pred) >= 0.5).astype(int)\n",
        "#     return accuracy_score(y_true, y_pred_binary), precision_score(y_true, y_pred_binary, zero_division=1), recall_score(y_true, y_pred_binary), roc_auc_score(y_true, y_pred)\n",
        "\n",
        "# print(\"Final Test Performance:\", evaluate(model, test_loader))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "9nzTioPX0Rxc",
        "outputId": "b617a256-8a70-4310-f398-8247ca213a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            filename  study_id  patient_id  \\\n",
              "0  mimic-iv-ecg/files/p1000/p10000032/s40689238/4...  40689238    10000032   \n",
              "1  mimic-iv-ecg/files/p1000/p10000032/s44458630/4...  44458630    10000032   \n",
              "2  mimic-iv-ecg/files/p1000/p10000032/s49036311/4...  49036311    10000032   \n",
              "3  mimic-iv-ecg/files/p1000/p10000117/s45090959/4...  45090959    10000117   \n",
              "4  mimic-iv-ecg/files/p1000/p10000117/s48446569/4...  48446569    10000117   \n",
              "\n",
              "              ecg_time  ed_stay_id  ed_hadm_id  hosp_hadm_id ed_diag_ed  \\\n",
              "0  2180-07-23 08:44:00         NaN         NaN           NaN         []   \n",
              "1  2180-07-23 09:54:00         NaN         NaN           NaN         []   \n",
              "2  2180-08-06 09:07:00         NaN         NaN    25742920.0         []   \n",
              "3  2181-03-04 17:14:00         NaN         NaN           NaN         []   \n",
              "4  2183-09-18 13:52:00         NaN         NaN           NaN         []   \n",
              "\n",
              "  ed_diag_hosp                                     hosp_diag_hosp  ...   age  \\\n",
              "0           []                                                 []  ...  52.0   \n",
              "1           []                                                 []  ...  52.0   \n",
              "2           []  ['J449', 'E875', 'Z21', 'R188', 'R197', 'E871'...  ...  52.0   \n",
              "3           []                                                 []  ...  55.0   \n",
              "4           []                                                 []  ...  57.0   \n",
              "\n",
              "  anchor_year anchor_age         dod  ecg_no_within_stay  ecg_taken_in_ed  \\\n",
              "0      2180.0       52.0  2180-09-09                  -1            False   \n",
              "1      2180.0       52.0  2180-09-09                  -1            False   \n",
              "2      2180.0       52.0  2180-09-09                   0            False   \n",
              "3      2174.0       48.0         NaN                  -1            False   \n",
              "4      2174.0       48.0         NaN                  -1            False   \n",
              "\n",
              "  ecg_taken_in_hosp  ecg_taken_in_ed_or_hosp  fold  strat_fold  \n",
              "0             False                    False    17          19  \n",
              "1             False                    False    17          19  \n",
              "2              True                     True    17          19  \n",
              "3             False                    False    18           5  \n",
              "4             False                    False    18           5  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4fc1d5fe-44bd-44f8-b6e9-5b5940b5069a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>study_id</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>ecg_time</th>\n",
              "      <th>ed_stay_id</th>\n",
              "      <th>ed_hadm_id</th>\n",
              "      <th>hosp_hadm_id</th>\n",
              "      <th>ed_diag_ed</th>\n",
              "      <th>ed_diag_hosp</th>\n",
              "      <th>hosp_diag_hosp</th>\n",
              "      <th>...</th>\n",
              "      <th>age</th>\n",
              "      <th>anchor_year</th>\n",
              "      <th>anchor_age</th>\n",
              "      <th>dod</th>\n",
              "      <th>ecg_no_within_stay</th>\n",
              "      <th>ecg_taken_in_ed</th>\n",
              "      <th>ecg_taken_in_hosp</th>\n",
              "      <th>ecg_taken_in_ed_or_hosp</th>\n",
              "      <th>fold</th>\n",
              "      <th>strat_fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mimic-iv-ecg/files/p1000/p10000032/s40689238/4...</td>\n",
              "      <td>40689238</td>\n",
              "      <td>10000032</td>\n",
              "      <td>2180-07-23 08:44:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>...</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2180.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2180-09-09</td>\n",
              "      <td>-1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mimic-iv-ecg/files/p1000/p10000032/s44458630/4...</td>\n",
              "      <td>44458630</td>\n",
              "      <td>10000032</td>\n",
              "      <td>2180-07-23 09:54:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>...</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2180.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2180-09-09</td>\n",
              "      <td>-1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mimic-iv-ecg/files/p1000/p10000032/s49036311/4...</td>\n",
              "      <td>49036311</td>\n",
              "      <td>10000032</td>\n",
              "      <td>2180-08-06 09:07:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25742920.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>['J449', 'E875', 'Z21', 'R188', 'R197', 'E871'...</td>\n",
              "      <td>...</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2180.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2180-09-09</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mimic-iv-ecg/files/p1000/p10000117/s45090959/4...</td>\n",
              "      <td>45090959</td>\n",
              "      <td>10000117</td>\n",
              "      <td>2181-03-04 17:14:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>...</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2174.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mimic-iv-ecg/files/p1000/p10000117/s48446569/4...</td>\n",
              "      <td>48446569</td>\n",
              "      <td>10000117</td>\n",
              "      <td>2183-09-18 13:52:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>...</td>\n",
              "      <td>57.0</td>\n",
              "      <td>2174.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4fc1d5fe-44bd-44f8-b6e9-5b5940b5069a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4fc1d5fe-44bd-44f8-b6e9-5b5940b5069a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4fc1d5fe-44bd-44f8-b6e9-5b5940b5069a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0a668d49-b7d4-4bea-9f77-3324c2b5fd93\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a668d49-b7d4-4bea-9f77-3324c2b5fd93')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0a668d49-b7d4-4bea-9f77-3324c2b5fd93 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def compress_npy(file_path):\n",
        "    \"\"\"Compresses a .npy file using numpy.savez_compressed.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the .npy file.\n",
        "    \"\"\"\n",
        "    if not file_path.endswith('.npy') or not os.path.exists(file_path):\n",
        "        raise ValueError(\"Invalid file path. Must be an existing .npy file.\")\n",
        "\n",
        "    data = np.load(file_path)\n",
        "    base_name = os.path.splitext(file_path)[0]\n",
        "    compressed_file_path = f\"{base_name}.npz\"\n",
        "    np.savez_compressed(compressed_file_path, data=data)\n",
        "\n",
        "    print(f\"Compressed file saved to: {compressed_file_path}\")\n",
        "    os.remove(file_path)\n",
        "    print(f\"Original file removed: {file_path}\")\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"memmap (1).npy\"\n",
        "arr = np.random.rand(100, 100)\n",
        "np.save(file_path, arr)\n",
        "compress_npy(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa7vVmKEQAmS",
        "outputId": "b0cfb714-4224-409c-ae3b-463612baf901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressed file saved to: memmap (1).npz\n",
            "Original file removed: memmap (1).npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memmap_meta = np.load(\"memmap_meta.npz\", allow_pickle=True)\n",
        "print(memmap_meta.files)  # Lists all stored keys\n",
        "\n",
        "# Peek inside each key\n",
        "for key in memmap_meta.files:\n",
        "    print(f\"{key}:\", memmap_meta[key])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3r_Lt9dQ2_9",
        "outputId": "0969e274-31f6-4ff0-fcaa-34db0ad2c787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['start', 'length', 'shape', 'dtype']\n",
            "start: [       0     1000     2000 ... 21646000 21647000 21648000]\n",
            "length: [1000 1000 1000 ... 1000 1000 1000]\n",
            "shape: [21649000       12]\n",
            "dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your stroke-labeled diagnosis dataframe\n",
        "diagnosis_df = pd.read_csv(\"records_w_diag_icd10 (1).csv\", low_memory=False, on_bad_lines='warn')\n",
        "\n",
        "# Load memmap metadata and actual ECG data\n",
        "memmap_meta = np.load(\"memmap_meta.npz\", allow_pickle=True)\n",
        "print(\"Meta keys:\", memmap_meta.files)\n",
        "\n",
        "memmap_data = np.load(\"memmap (1).npz\", allow_pickle=True)\n",
        "print(\"ECG data keys:\", memmap_data.files)\n",
        "\n",
        "# Show a sample of your diagnosis data\n",
        "print(diagnosis_df.head())\n",
        "\n",
        "# Inspect ECG data shape\n",
        "ecg_array = memmap_data[\"data\"]\n",
        "print(\"ECG shape:\", ecg_array.shape)  # Should be (num_samples, length, channels) or similar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "IQIix7S2QTNG",
        "outputId": "a474b6a5-cf12-462f-ed70-5702f960bf83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 99886",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e4610448d9f6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load your stroke-labeled diagnosis dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdiagnosis_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"records_w_diag_icd10 (1).csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load memmap metadata and actual ECG data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 99886"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diagnosis_df = diagnosis_df.iloc[:ecg_array.shape[0]].reset_index(drop=True)\n",
        "def has_stroke_icd10(icd_list):\n",
        "    stroke_prefixes = [f\"I6{i}\" for i in range(10)]\n",
        "    if isinstance(icd_list, str):\n",
        "        try:\n",
        "            icd_list = eval(icd_list)\n",
        "        except:\n",
        "            return 0\n",
        "    return int(any(code[:3] in stroke_prefixes for code in icd_list))\n",
        "\n",
        "diagnosis_df[\"stroke_label\"] = diagnosis_df[\"hosp_diag_hosp\"].apply(has_stroke_icd10)\n",
        "\n",
        "print(\"Class balance:\\n\", diagnosis_df[\"stroke_label\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzAqzrprQVcO",
        "outputId": "9b1a4b09-5367-43c1-8eef-85559ac71a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class balance:\n",
            " stroke_label\n",
            "0    99\n",
            "1     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spJFYCThVZ-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use whole dataset with loss weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Count positives and negatives\n",
        "num_pos = diagnosis_df[\"stroke_label\"].sum()\n",
        "num_neg = len(diagnosis_df) - num_pos\n",
        "\n",
        "# Compute pos_weight (how much more important a stroke is than a non-stroke)\n",
        "pos_weight = torch.tensor([num_neg / num_pos], dtype=torch.float32)\n",
        "\n",
        "# Use in your loss function\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
      ],
      "metadata": {
        "id": "UMQ_UXVfSwlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use downsampling\n",
        "# Separate classes\n",
        "stroke_df = diagnosis_df[diagnosis_df[\"stroke_label\"] == 1]\n",
        "nonstroke_df = diagnosis_df[diagnosis_df[\"stroke_label\"] == 0]\n",
        "\n",
        "# Random sample non-stroke rows to match stroke count\n",
        "nonstroke_sampled = nonstroke_df.sample(n=len(stroke_df), random_state=42)\n",
        "\n",
        "# Combine\n",
        "balanced_df = pd.concat([stroke_df, nonstroke_sampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Balanced dataset size: {len(balanced_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlV7YusdTDZI",
        "outputId": "fe71106a-3e4f-4f5c-9f6a-420c5ee4be3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced dataset size: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPJzl6m8Vk_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, df, ecg_data):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.ecg_data = ecg_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ecg = self.ecg_data[idx]              # shape: [5000, 12] or flattened\n",
        "        label = float(self.df.loc[idx, \"stroke_label\"])\n",
        "        return torch.tensor(ecg, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "gI_o_6NgTMeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleECG1DCNN(nn.Module):\n",
        "    def __init__(self, input_channels=12, sequence_length=5000):\n",
        "        super(SimpleECG1DCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=7, stride=1, padding=3)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.pool3 = nn.AdaptiveAvgPool1d(1)  # Output shape: [B, 128, 1]\n",
        "\n",
        "        self.fc = nn.Linear(128, 1)  # Binary output (logit)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape: [B, Seq, C] -> [B, C, Seq]\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)  # Logit output (no sigmoid here)\n"
      ],
      "metadata": {
        "id": "VefyPw7UUGyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, df, ecg_data):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.ecg_data = ecg_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ecg = self.ecg_data[idx]  # Shape: [5000, 12] or [length, channels]\n",
        "        label = float(self.df.loc[idx, \"stroke_label\"])\n",
        "        return torch.tensor(ecg, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "PwAaaBL1WnmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diagnosis_df = diagnosis_df.iloc[:len(ecg_array)].reset_index(drop=True)\n",
        "balanced_df = balanced_df.iloc[:len(ecg_array)].reset_index(drop=True)\n",
        "\n",
        "# Proceed to create datasets\n",
        "full_dataset = ECGDataset(diagnosis_df, ecg_array)\n",
        "balanced_dataset = ECGDataset(balanced_df, ecg_array)"
      ],
      "metadata": {
        "id": "qMbkl_gaXBd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device).unsqueeze(1)  # Make y shape [B, 1]\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "nLKle0f2UKpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ensure 'stroke_label' column exists ---\n",
        "if \"stroke_label\" not in diagnosis_df.columns:\n",
        "    def has_stroke_icd10(icd_list):\n",
        "        valid_prefixes = [f\"I6{i}\" for i in range(10)]  # I60 to I69\n",
        "        if isinstance(icd_list, str):\n",
        "            try:\n",
        "                icd_list = eval(icd_list)\n",
        "            except:\n",
        "                return 0\n",
        "        return int(any(code[:3] in valid_prefixes for code in icd_list))\n",
        "\n",
        "    diagnosis_df[\"stroke_label\"] = diagnosis_df[\"hosp_diag_hosp\"].apply(has_stroke_icd10)\n"
      ],
      "metadata": {
        "id": "2Hq079oKYsNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleECG1DCNN().to(device)\n",
        "\n",
        "# For Strategy 1 (weighted loss)\n",
        "num_pos = diagnosis_df[\"stroke_label\"].sum()\n",
        "num_neg = len(diagnosis_df) - num_pos\n",
        "pos_weight = torch.tensor([num_neg / num_pos], dtype=torch.float32).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop (example for 5 epochs)\n",
        "for epoch in range(5):\n",
        "    loss = train(model, full_loader, criterion, optimizer, device)\n",
        "    print(f\"Epoch {epoch+1} - Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "u1_NyMhKUMvE",
        "outputId": "f2bcaddd-dd9d-4c69-e437-30a39cedbb58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'stroke_label'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'stroke_label'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-17a40de77e77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Training loop (example for 5 epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} - Loss: {loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-7399381bf475>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make y shape [B, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-c9e8dac64bae>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mecg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mecg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m              \u001b[0;31m# shape: [5000, 12] or flattened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stroke_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mecg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4212\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4214\u001b[0;31m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4215\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4636\u001b[0m             \u001b[0;31m#  pending resolution of GH#33047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4638\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4639\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'stroke_label'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. VERIFY FILE INTEGRITY\n",
        "file_path = \"memmap (1).npz\"\n",
        "file_size = os.path.getsize(file_path)\n",
        "expected_size = 21649000 * 12 * 4  # 1.04GB for float32\n",
        "\n",
        "print(f\"File size: {file_size/1e6:.2f}MB | Expected: {expected_size/1e6:.2f}MB\")\n",
        "\n",
        "# 2. ATTEMPT PROPER LOADING\n",
        "try:\n",
        "    # Load with mmap for large files\n",
        "    with np.load(file_path, mmap_mode='r') as data:\n",
        "        if 'data' in data:\n",
        "            ecg_data = data['data']\n",
        "            print(f\"Loaded shape: {ecg_data.shape}\")\n",
        "        else:\n",
        "            print(\"No 'data' array found in file. Available keys:\", list(data.keys()))\n",
        "except Exception as e:\n",
        "    print(f\"Load error: {e}\")\n",
        "\n",
        "# 3. DATA RECOVERY STRATEGY\n",
        "if 'ecg_data' not in locals() or ecg_data.shape[0] < 1000:\n",
        "    print(\"\\n⚠️ Using partial data recovery approach\")\n",
        "\n",
        "    # Try loading what we can\n",
        "    try:\n",
        "        with np.load(file_path, allow_pickle=True) as data:\n",
        "            all_arrays = {k: data[k] for k in data.files}\n",
        "            print(\"Found arrays:\", list(all_arrays.keys()))\n",
        "\n",
        "            # Find the largest array\n",
        "            ecg_data = max(all_arrays.values(), key=lambda x: x.size)\n",
        "            print(f\"Using largest array found: {ecg_data.shape}\")\n",
        "    except:\n",
        "        print(\"Could not recover any data. Creating synthetic placeholder...\")\n",
        "        ecg_data = np.zeros((100, 12), dtype=np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCjVuR7l8PxC",
        "outputId": "cfea8721-c99d-445d-8105-7e8b8a07c5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 0.08MB | Expected: 1039.15MB\n",
            "Loaded shape: (100, 100)\n",
            "\n",
            "⚠️ Using partial data recovery approach\n",
            "Found arrays: ['data']\n",
            "Using largest array found: (100, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- COMPLETE ECG STROKE PREDICTION SYSTEM --------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import os\n",
        "import re\n",
        "\n",
        "# -------------------- 1. DATA LOADING --------------------\n",
        "def load_ecg_data():\n",
        "    file_path = \"memmap (1).npy\"\n",
        "    try:\n",
        "        data = np.fromfile(file_path, dtype=np.float32)\n",
        "        ecg_array = data.reshape(-1, 3000)\n",
        "        print(\"✅ Loaded ECG data from .npy file\")\n",
        "        print(\"Shape:\", ecg_array.shape)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        ecg_array = scaler.fit_transform(ecg_array)\n",
        "        ecg_array = np.nan_to_num(ecg_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        return ecg_array\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Could not load ECG data: {e}\")\n",
        "\n",
        "def load_diagnosis_data(csv_path, expected_rows=None):\n",
        "    print(f\"\\n🔍 Loading diagnosis data from {csv_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, low_memory=False)\n",
        "        if expected_rows:\n",
        "            df = df.iloc[:expected_rows].reset_index(drop=True)\n",
        "            print(f\"Aligned to {expected_rows} rows to match ECG data\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load diagnosis data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# -------------------- 2. LABEL CREATION --------------------\n",
        "def create_stroke_labels(df):\n",
        "    def has_stroke(icd_list):\n",
        "        pattern = re.compile(r\"^I6[0-9]\")\n",
        "        if isinstance(icd_list, str):\n",
        "            try:\n",
        "                icd_list = eval(icd_list)\n",
        "            except:\n",
        "                return 0\n",
        "        if isinstance(icd_list, list):\n",
        "            return int(any(pattern.match(code) for code in icd_list if isinstance(code, str)))\n",
        "        return 0\n",
        "\n",
        "    df[\"stroke_label\"] = df[\"hosp_diag_hosp\"].apply(has_stroke)\n",
        "    return df\n",
        "\n",
        "# -------------------- 3. DATASET + MODEL --------------------\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n",
        "\n",
        "class ECGCNNClassifier(nn.Module):\n",
        "    def __init__(self, input_length=3000):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "            nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "            nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape for CNN (batch_size, channels, sequence_length)\n",
        "        x = x.view(x.size(0), 1, -1)\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc_layers(x)\n",
        "\n",
        "def extract_ecg_features(ecg_array):\n",
        "    \"\"\"Extract relevant features from ECG data\"\"\"\n",
        "    features = []\n",
        "\n",
        "    for ecg in ecg_array:\n",
        "        # Simple statistical features\n",
        "        mean = np.mean(ecg)\n",
        "        std = np.std(ecg)\n",
        "        min_val = np.min(ecg)\n",
        "        max_val = np.max(ecg)\n",
        "        p2p = max_val - min_val\n",
        "\n",
        "        # Simple frequency domain features using FFT\n",
        "        fft_vals = np.abs(np.fft.rfft(ecg))\n",
        "        fft_freq = np.fft.rfftfreq(len(ecg))\n",
        "        dominant_freq = fft_freq[np.argmax(fft_vals)]\n",
        "\n",
        "        # Combine features\n",
        "        feature_vector = np.array([\n",
        "            mean, std, min_val, max_val, p2p,\n",
        "            dominant_freq, np.mean(fft_vals), np.std(fft_vals)\n",
        "        ])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "# -------------------- 4. LOSS + TRAINING --------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=0.5, pos_weight=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Clip inputs to prevent extreme values\n",
        "        inputs = torch.clamp(inputs, -50, 50)\n",
        "\n",
        "        # Using BCE with logits for numerical stability\n",
        "        bce = nn.functional.binary_cross_entropy_with_logits(\n",
        "            inputs, targets,\n",
        "            pos_weight=self.pos_weight,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "        # Safe exponential calculation\n",
        "        pt = torch.exp(-torch.clamp(bce, max=50))\n",
        "\n",
        "        # Calculate focal term with safety checks\n",
        "        focal_term = (1 - pt + 1e-7) ** self.gamma\n",
        "\n",
        "        # Return mean of the loss\n",
        "        loss = focal_term * bce\n",
        "\n",
        "        # Check for NaN and replace with zero\n",
        "        loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device).unsqueeze(1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Add gradient clipping here\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion, device, threshold=0.15):\n",
        "    model.eval()\n",
        "    all_preds, all_probs, all_labels = [], [], []\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device).unsqueeze(1)\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs > threshold).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "            total_loss += loss.item()\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    return total_loss / len(loader), np.array(all_preds), np.array(all_labels), auc, np.array(all_probs)\n",
        "\n",
        "def find_best_threshold(probs, labels):\n",
        "    \"\"\"Find threshold that maximizes F1 score\"\"\"\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "\n",
        "    for threshold in np.arange(0.05, 0.95, 0.05):\n",
        "        preds = (probs > threshold).astype(int)\n",
        "        f1 = f1_score(labels, preds)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "\n",
        "    print(f\"✅ Best threshold: {best_threshold:.2f} with F1 score: {best_f1:.4f}\")\n",
        "    return best_threshold\n",
        "\n",
        "# -------------------- 5. MAIN TRAINING --------------------\n",
        "def run_training(X, y, n_splits=5, epochs=20, batch_size=128):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    results = []\n",
        "    sampler = SMOTE(random_state=42)\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    all_val_probs = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
        "        print(f\"\\n=== FOLD {fold+1}/{n_splits} ===\")\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Resample with SMOTE\n",
        "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
        "        print(f\"✅ Resampled training set — Class balance: {Counter(y_train)}\")\n",
        "\n",
        "        # Clean\n",
        "        X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        # Optional re-normalization\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        # Dataloaders\n",
        "        train_loader = DataLoader(ECGDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(ECGDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "        model = ECGCNNClassifier(X.shape[1]).to(device)\n",
        "        pos_count = y_train.sum()\n",
        "        neg_count = len(y_train) - pos_count\n",
        "        pos_weight = torch.tensor([neg_count / pos_count if pos_count > 0 else 1.0]).to(device)\n",
        "\n",
        "        criterion = FocalLoss(gamma=0.5, pos_weight=pos_weight)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "        # Use cosine annealing scheduler with restarts\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=epochs//2, eta_min=1e-6)\n",
        "\n",
        "        # Add validation tracking\n",
        "        best_auc = 0\n",
        "        best_model = None\n",
        "        best_val_probs = None\n",
        "        best_val_labels = None\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            scheduler.step()\n",
        "\n",
        "            # Validate every epoch\n",
        "            val_loss, val_preds, val_labels, val_auc, val_probs = evaluate(\n",
        "                model, test_loader, criterion, device, threshold=0.15\n",
        "            )\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | AUC: {val_auc:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_auc > best_auc:\n",
        "                best_auc = val_auc\n",
        "                best_model = copy.deepcopy(model)\n",
        "                best_val_probs = val_probs\n",
        "                best_val_labels = val_labels\n",
        "                print(f\"✅ New best model saved! AUC: {val_auc:.4f}\")\n",
        "\n",
        "        # Store validation probabilities for threshold optimization\n",
        "        all_val_probs.extend(best_val_probs)\n",
        "        all_val_labels.extend(best_val_labels)\n",
        "\n",
        "        # Find best threshold for final evaluation\n",
        "        if fold == n_splits - 1:  # On last fold\n",
        "            best_threshold = find_best_threshold(np.array(all_val_probs), np.array(all_val_labels))\n",
        "        else:\n",
        "            best_threshold = 0.15  # Default threshold\n",
        "\n",
        "        # Use best model for final evaluation\n",
        "        test_loss, preds, labels, auc, probs = evaluate(best_model, test_loader, criterion, device, threshold=best_threshold)\n",
        "\n",
        "        print(f\"→ Predicted Positives: {int(preds.sum())} / {len(preds)}\")\n",
        "        print(f\"→ Actual Positives: {int(labels.sum())} / {len(labels)}\")\n",
        "        print(f\"→ ROC-AUC: {auc:.4f}\")\n",
        "\n",
        "        unique_preds = np.unique(preds, return_counts=True)\n",
        "        print(f\"→ Unique predictions: {dict(zip(unique_preds[0], unique_preds[1]))}\")\n",
        "\n",
        "        # Check if preds array has been modified\n",
        "        print(f\"→ Shape of preds array: {preds.shape}\")\n",
        "        print(f\"→ Shape of labels array: {labels.shape}\")\n",
        "\n",
        "        report = classification_report(labels, preds, target_names=['Non-Stroke', 'Stroke'], output_dict=True, zero_division=0)\n",
        "\n",
        "        results.append({\n",
        "            'fold': fold+1,\n",
        "            'test_loss': test_loss,\n",
        "            'roc_auc': auc,\n",
        "            'best_threshold': best_threshold,\n",
        "            'report': report,\n",
        "            'model': best_model\n",
        "        })\n",
        "\n",
        "        # Save model checkpoint\n",
        "        torch.save({\n",
        "            'model_state_dict': best_model.state_dict(),\n",
        "            'threshold': best_threshold,\n",
        "            'auc': auc,\n",
        "        }, f'ecg_stroke_model_fold{fold+1}.pt')\n",
        "        print(f\"✅ Model saved to ecg_stroke_model_fold{fold+1}.pt\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------- 6. EXECUTION --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    ecg_data = load_ecg_data()\n",
        "    diagnosis_df = load_diagnosis_data(\"records_w_diag_icd10 (1).csv\", len(ecg_data))\n",
        "    diagnosis_df = create_stroke_labels(diagnosis_df)\n",
        "\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(diagnosis_df[\"stroke_label\"].value_counts())\n",
        "\n",
        "    results = run_training(\n",
        "        ecg_data,\n",
        "        diagnosis_df[\"stroke_label\"].values,\n",
        "        n_splits=5,\n",
        "        epochs=5,\n",
        "        batch_size=128\n",
        "    )\n",
        "\n",
        "    print(\"\\n📊 Final Results:\")\n",
        "    avg_auc = 0\n",
        "    for res in results:\n",
        "        print(f\"\\nFold {res['fold']}:\")\n",
        "        print(f\"Test Loss: {res['test_loss']:.4f}\")\n",
        "        print(f\"ROC AUC: {res['roc_auc']:.4f}\")\n",
        "        print(f\"Best Threshold: {res['best_threshold']:.4f}\")\n",
        "        avg_auc += res['roc_auc']\n",
        "\n",
        "        # Check both key formats that might be used in the report\n",
        "        if '1' in res['report']:\n",
        "            print(f\"Stroke Precision: {res['report']['1']['precision']:.4f}\")\n",
        "            print(f\"Stroke Recall: {res['report']['1']['recall']:.4f}\")\n",
        "            print(f\"Stroke F1 Score: {res['report']['1']['f1-score']:.4f}\")\n",
        "        elif 'Stroke' in res['report']:\n",
        "            print(f\"Stroke Precision: {res['report']['Stroke']['precision']:.4f}\")\n",
        "            print(f\"Stroke Recall: {res['report']['Stroke']['recall']:.4f}\")\n",
        "            print(f\"Stroke F1 Score: {res['report']['Stroke']['f1-score']:.4f}\")\n",
        "        else:\n",
        "            # Check the actual predictions count\n",
        "            preds_count = res.get('preds_sum', 'unknown')\n",
        "            print(f\"⚠️ No stroke predictions found in report. Predicted positives: {preds_count}\")\n",
        "\n",
        "    print(f\"\\n✅ Average ROC AUC across all folds: {avg_auc / len(results):.4f}\")\n",
        "\n",
        "def predict_with_ensemble(ecg_data, model_paths, threshold=None):\n",
        "    \"\"\"\n",
        "    Make predictions using an ensemble of models\n",
        "\n",
        "    Args:\n",
        "        ecg_data: ECG data to predict on (already preprocessed)\n",
        "        model_paths: List of paths to model checkpoints\n",
        "        threshold: Optional prediction threshold (if None, uses saved threshold)\n",
        "\n",
        "    Returns:\n",
        "        Average probabilities and binary predictions\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    all_probs = []\n",
        "\n",
        "    for model_path in model_paths:\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model = ECGCNNClassifier(ecg_data.shape[1]).to(device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.eval()\n",
        "\n",
        "        # Use the model's saved threshold if none provided\n",
        "        model_threshold = threshold if threshold is not None else checkpoint.get('threshold', 0.5)\n",
        "\n",
        "        # Prepare data\n",
        "        dataset = ECGDataset(ecg_data, np.zeros(len(ecg_data)))\n",
        "        loader = DataLoader(dataset, batch_size=128)\n",
        "\n",
        "        # Get predictions\n",
        "        probs = []\n",
        "        with torch.no_grad():\n",
        "            for X, _ in loader:\n",
        "                X = X.to(device)\n",
        "                outputs = model(X)\n",
        "                batch_probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "                probs.extend(batch_probs)\n",
        "\n",
        "        all_probs.append(np.array(probs))\n",
        "\n",
        "    # Average probabilities from all models\n",
        "    avg_probs = np.mean(np.array(all_probs), axis=0)\n",
        "\n",
        "    # Use threshold to get binary predictions\n",
        "    final_threshold = threshold if threshold is not None else 0.5\n",
        "    binary_preds = (avg_probs > final_threshold).astype(int)\n",
        "\n",
        "    return avg_probs, binary_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5oRxZ2KXnA_",
        "outputId": "0a752db7-0281-4069-d200-25f8ef0a81a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded ECG data from .npy file\n",
            "Shape: (86596, 3000)\n",
            "\n",
            "🔍 Loading diagnosis data from records_w_diag_icd10 (1).csv\n",
            "Aligned to 86596 rows to match ECG data\n",
            "\n",
            "Class Distribution:\n",
            "stroke_label\n",
            "0    83366\n",
            "1     3230\n",
            "Name: count, dtype: int64\n",
            "Using device: cuda\n",
            "\n",
            "=== FOLD 1/5 ===\n",
            "✅ Resampled training set — Class balance: Counter({np.int64(0): 66692, np.int64(1): 66692})\n",
            "Epoch 1/5 | Train Loss: 0.3785 | Val Loss: 0.3457 | AUC: 0.5451\n",
            "✅ New best model saved! AUC: 0.5451\n",
            "Epoch 2/5 | Train Loss: 0.3404 | Val Loss: 0.3061 | AUC: 0.5665\n",
            "✅ New best model saved! AUC: 0.5665\n",
            "Epoch 3/5 | Train Loss: 0.3294 | Val Loss: 0.3897 | AUC: 0.5670\n",
            "✅ New best model saved! AUC: 0.5670\n",
            "Epoch 4/5 | Train Loss: 0.3262 | Val Loss: 0.3262 | AUC: 0.5763\n",
            "✅ New best model saved! AUC: 0.5763\n",
            "Epoch 5/5 | Train Loss: 0.3139 | Val Loss: 0.3147 | AUC: 0.5943\n",
            "✅ New best model saved! AUC: 0.5943\n",
            "→ Predicted Positives: 13892 / 17320\n",
            "→ Actual Positives: 646 / 17320\n",
            "→ ROC-AUC: 0.5943\n",
            "→ Unique predictions: {np.False_: np.int64(3428), np.True_: np.int64(13892)}\n",
            "→ Shape of preds array: (17320, 1)\n",
            "→ Shape of labels array: (17320, 1)\n",
            "✅ Model saved to ecg_stroke_model_fold1.pt\n",
            "\n",
            "=== FOLD 2/5 ===\n",
            "✅ Resampled training set — Class balance: Counter({np.int64(0): 66693, np.int64(1): 66693})\n",
            "Epoch 1/5 | Train Loss: 0.3805 | Val Loss: 0.3789 | AUC: 0.5611\n",
            "✅ New best model saved! AUC: 0.5611\n",
            "Epoch 2/5 | Train Loss: 0.3423 | Val Loss: 0.3845 | AUC: 0.5736\n",
            "✅ New best model saved! AUC: 0.5736\n",
            "Epoch 3/5 | Train Loss: 0.3322 | Val Loss: 0.3939 | AUC: 0.5732\n",
            "Epoch 4/5 | Train Loss: 0.3270 | Val Loss: 0.3205 | AUC: 0.5853\n",
            "✅ New best model saved! AUC: 0.5853\n",
            "Epoch 5/5 | Train Loss: 0.3175 | Val Loss: 0.3460 | AUC: 0.5902\n",
            "✅ New best model saved! AUC: 0.5902\n",
            "→ Predicted Positives: 14595 / 17319\n",
            "→ Actual Positives: 646 / 17319\n",
            "→ ROC-AUC: 0.5902\n",
            "→ Unique predictions: {np.False_: np.int64(2724), np.True_: np.int64(14595)}\n",
            "→ Shape of preds array: (17319, 1)\n",
            "→ Shape of labels array: (17319, 1)\n",
            "✅ Model saved to ecg_stroke_model_fold2.pt\n",
            "\n",
            "=== FOLD 3/5 ===\n",
            "✅ Resampled training set — Class balance: Counter({np.int64(0): 66693, np.int64(1): 66693})\n",
            "Epoch 1/5 | Train Loss: 0.3933 | Val Loss: 0.4112 | AUC: 0.5615\n",
            "✅ New best model saved! AUC: 0.5615\n",
            "Epoch 2/5 | Train Loss: 0.3508 | Val Loss: 0.4325 | AUC: 0.5826\n",
            "✅ New best model saved! AUC: 0.5826\n",
            "Epoch 3/5 | Train Loss: 0.3374 | Val Loss: 0.3736 | AUC: 0.5817\n",
            "Epoch 4/5 | Train Loss: 0.3309 | Val Loss: 0.3076 | AUC: 0.5951\n",
            "✅ New best model saved! AUC: 0.5951\n",
            "Epoch 5/5 | Train Loss: 0.3176 | Val Loss: 0.2318 | AUC: 0.6139\n",
            "✅ New best model saved! AUC: 0.6139\n",
            "→ Predicted Positives: 12118 / 17319\n",
            "→ Actual Positives: 646 / 17319\n",
            "→ ROC-AUC: 0.6139\n",
            "→ Unique predictions: {np.False_: np.int64(5201), np.True_: np.int64(12118)}\n",
            "→ Shape of preds array: (17319, 1)\n",
            "→ Shape of labels array: (17319, 1)\n",
            "✅ Model saved to ecg_stroke_model_fold3.pt\n",
            "\n",
            "=== FOLD 4/5 ===\n",
            "✅ Resampled training set — Class balance: Counter({np.int64(0): 66693, np.int64(1): 66693})\n",
            "Epoch 1/5 | Train Loss: 0.3865 | Val Loss: 0.3908 | AUC: 0.5391\n",
            "✅ New best model saved! AUC: 0.5391\n",
            "Epoch 2/5 | Train Loss: 0.3490 | Val Loss: 0.3207 | AUC: 0.5516\n",
            "✅ New best model saved! AUC: 0.5516\n",
            "Epoch 3/5 | Train Loss: 0.3383 | Val Loss: 0.3595 | AUC: 0.5535\n",
            "✅ New best model saved! AUC: 0.5535\n",
            "Epoch 4/5 | Train Loss: 0.3341 | Val Loss: 0.3993 | AUC: 0.5608\n",
            "✅ New best model saved! AUC: 0.5608\n",
            "Epoch 5/5 | Train Loss: 0.3222 | Val Loss: 0.2722 | AUC: 0.5749\n",
            "✅ New best model saved! AUC: 0.5749\n",
            "→ Predicted Positives: 13660 / 17319\n",
            "→ Actual Positives: 646 / 17319\n",
            "→ ROC-AUC: 0.5749\n",
            "→ Unique predictions: {np.False_: np.int64(3659), np.True_: np.int64(13660)}\n",
            "→ Shape of preds array: (17319, 1)\n",
            "→ Shape of labels array: (17319, 1)\n",
            "✅ Model saved to ecg_stroke_model_fold4.pt\n",
            "\n",
            "=== FOLD 5/5 ===\n",
            "✅ Resampled training set — Class balance: Counter({np.int64(0): 66693, np.int64(1): 66693})\n",
            "Epoch 1/5 | Train Loss: 0.3852 | Val Loss: 0.3074 | AUC: 0.5417\n",
            "✅ New best model saved! AUC: 0.5417\n",
            "Epoch 2/5 | Train Loss: 0.3464 | Val Loss: 0.3302 | AUC: 0.5626\n",
            "✅ New best model saved! AUC: 0.5626\n",
            "Epoch 3/5 | Train Loss: 0.3336 | Val Loss: 0.3982 | AUC: 0.5627\n",
            "✅ New best model saved! AUC: 0.5627\n",
            "Epoch 4/5 | Train Loss: 0.3279 | Val Loss: 0.3674 | AUC: 0.5833\n",
            "✅ New best model saved! AUC: 0.5833\n",
            "Epoch 5/5 | Train Loss: 0.3143 | Val Loss: 0.3504 | AUC: 0.5886\n",
            "✅ New best model saved! AUC: 0.5886\n",
            "✅ Best threshold: 0.35 with F1 score: 0.0903\n",
            "→ Predicted Positives: 7536 / 17319\n",
            "→ Actual Positives: 646 / 17319\n",
            "→ ROC-AUC: 0.5886\n",
            "→ Unique predictions: {np.False_: np.int64(9783), np.True_: np.int64(7536)}\n",
            "→ Shape of preds array: (17319, 1)\n",
            "→ Shape of labels array: (17319, 1)\n",
            "✅ Model saved to ecg_stroke_model_fold5.pt\n",
            "\n",
            "📊 Final Results:\n",
            "\n",
            "Fold 1:\n",
            "Test Loss: 0.3147\n",
            "ROC AUC: 0.5943\n",
            "Best Threshold: 0.1500\n",
            "Stroke Precision: 0.0417\n",
            "Stroke Recall: 0.8963\n",
            "Stroke F1 Score: 0.0797\n",
            "\n",
            "Fold 2:\n",
            "Test Loss: 0.3460\n",
            "ROC AUC: 0.5902\n",
            "Best Threshold: 0.1500\n",
            "Stroke Precision: 0.0408\n",
            "Stroke Recall: 0.9226\n",
            "Stroke F1 Score: 0.0782\n",
            "\n",
            "Fold 3:\n",
            "Test Loss: 0.2318\n",
            "ROC AUC: 0.6139\n",
            "Best Threshold: 0.1500\n",
            "Stroke Precision: 0.0449\n",
            "Stroke Recall: 0.8421\n",
            "Stroke F1 Score: 0.0852\n",
            "\n",
            "Fold 4:\n",
            "Test Loss: 0.2722\n",
            "ROC AUC: 0.5749\n",
            "Best Threshold: 0.1500\n",
            "Stroke Precision: 0.0413\n",
            "Stroke Recall: 0.8731\n",
            "Stroke F1 Score: 0.0788\n",
            "\n",
            "Fold 5:\n",
            "Test Loss: 0.3504\n",
            "ROC AUC: 0.5886\n",
            "Best Threshold: 0.3500\n",
            "Stroke Precision: 0.0474\n",
            "Stroke Recall: 0.5526\n",
            "Stroke F1 Score: 0.0873\n",
            "\n",
            "✅ Average ROC AUC across all folds: 0.5924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- ENHANCED ECG STROKE PREDICTION SYSTEM --------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_recall_curve, average_precision_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "import os\n",
        "import re\n",
        "!pip install PyWavelets\n",
        "import pywt\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "# -------------------- 1. DATA LOADING AND PREPROCESSING --------------------\n",
        "def load_ecg_data():\n",
        "    file_path = \"memmap (1).npy\"\n",
        "    try:\n",
        "        data = np.fromfile(file_path, dtype=np.float32)\n",
        "        ecg_array = data.reshape(-1, 3000)\n",
        "        print(\"✅ Loaded ECG data from .npy file\")\n",
        "        print(\"Shape:\", ecg_array.shape)\n",
        "        return ecg_array\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Could not load ECG data: {e}\")\n",
        "\n",
        "def load_diagnosis_data(csv_path, expected_rows=None):\n",
        "    print(f\"\\n🔍 Loading diagnosis data from {csv_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, low_memory=False)\n",
        "        if expected_rows:\n",
        "            df = df.iloc[:expected_rows].reset_index(drop=True)\n",
        "            print(f\"Aligned to {expected_rows} rows to match ECG data\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load diagnosis data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def preprocess_ecg(ecg_array, wavelet_transform=True):\n",
        "    \"\"\"Apply advanced preprocessing to ECG data\"\"\"\n",
        "    print(\"Preprocessing ECG data...\")\n",
        "\n",
        "    # 1. Remove baseline wander (high-pass filter via wavelet)\n",
        "    if wavelet_transform:\n",
        "        print(\"Applying wavelet denoising...\")\n",
        "        # Process in batches to avoid memory issues\n",
        "        batch_size = 1000\n",
        "        processed_data = []\n",
        "\n",
        "        for i in range(0, len(ecg_array), batch_size):\n",
        "            batch = ecg_array[i:i+batch_size]\n",
        "            processed_batch = []\n",
        "\n",
        "            for signal in batch:\n",
        "                # Handle invalid values before wavelet transform\n",
        "                signal = np.nan_to_num(signal, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                # Determine appropriate wavelet level based on signal length\n",
        "                level = min(5, pywt.dwt_max_level(len(signal), 'db4'))\n",
        "                coeffs = pywt.wavedec(signal, 'db4', level=level)\n",
        "\n",
        "                # Handle coefficients with epsilon to prevent division by zero\n",
        "                for j in range(1, len(coeffs)):\n",
        "                    std_val = np.std(coeffs[j]) + 1e-10\n",
        "                    coeffs[j] = pywt.threshold(coeffs[j], std_val/2, mode='soft')\n",
        "\n",
        "                # Reconstruct signal\n",
        "                reconstructed = pywt.waverec(coeffs, 'db4')\n",
        "\n",
        "                # Adjust length to match original if needed\n",
        "                if len(reconstructed) > len(signal):\n",
        "                    reconstructed = reconstructed[:len(signal)]\n",
        "                else:\n",
        "                    reconstructed = np.pad(reconstructed, (0, len(signal) - len(reconstructed)))\n",
        "\n",
        "                processed_batch.append(reconstructed)\n",
        "\n",
        "            processed_data.extend(processed_batch)\n",
        "\n",
        "        ecg_array = np.array(processed_data)\n",
        "\n",
        "    # 2. Handle outliers and invalid values before standardization\n",
        "    ecg_array = np.nan_to_num(ecg_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # 3. Standardize each signal carefully to avoid division by zero\n",
        "    print(\"Standardizing signals...\")\n",
        "    standardized_signals = []\n",
        "\n",
        "    for signal in ecg_array:\n",
        "        # Calculate mean and std safely\n",
        "        mean = np.mean(signal)\n",
        "        std = np.std(signal)\n",
        "\n",
        "        # If std is zero or very close to zero, use a small constant instead\n",
        "        if std < 1e-10:\n",
        "            standardized = signal - mean  # Just center the data if no variance\n",
        "        else:\n",
        "            standardized = (signal - mean) / std\n",
        "\n",
        "        standardized_signals.append(standardized)\n",
        "\n",
        "    ecg_array = np.array(standardized_signals)\n",
        "\n",
        "    # 4. Clip extreme values (beyond 5 std)\n",
        "    for i in range(len(ecg_array)):\n",
        "        # Re-calculate mean and std for the standardized signal\n",
        "        mean, std = np.mean(ecg_array[i]), np.std(ecg_array[i])\n",
        "        # Only clip if std is not near zero\n",
        "        if std > 1e-10:\n",
        "            ecg_array[i] = np.clip(ecg_array[i], mean - 5*std, mean + 5*std)\n",
        "\n",
        "    # 5. Final check for any remaining invalid values\n",
        "    ecg_array = np.nan_to_num(ecg_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    print(\"✅ ECG preprocessing complete\")\n",
        "    return ecg_array\n",
        "\n",
        "# -------------------- 2. LABEL CREATION --------------------\n",
        "def create_stroke_labels(df):\n",
        "    def has_stroke(icd_list):\n",
        "        pattern = re.compile(r\"^I6[0-9]\")\n",
        "        if isinstance(icd_list, str):\n",
        "            try:\n",
        "                icd_list = eval(icd_list)\n",
        "            except:\n",
        "                return 0\n",
        "        if isinstance(icd_list, list):\n",
        "            return int(any(pattern.match(code) for code in icd_list if isinstance(code, str)))\n",
        "        return 0\n",
        "\n",
        "    df[\"stroke_label\"] = df[\"hosp_diag_hosp\"].apply(has_stroke)\n",
        "    return df\n",
        "\n",
        "# -------------------- 3. DATASET AND FEATURE EXTRACTION --------------------\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n",
        "\n",
        "def extract_ecg_features(ecg_array):\n",
        "    \"\"\"Extract clinically relevant features from ECG data\"\"\"\n",
        "    print(\"Extracting ECG features...\")\n",
        "    features = []\n",
        "\n",
        "    for ecg in tqdm(ecg_array):\n",
        "        # Time domain features\n",
        "        mean = np.mean(ecg)\n",
        "        std = np.std(ecg)\n",
        "        min_val = np.min(ecg)\n",
        "        max_val = np.max(ecg)\n",
        "        p2p = max_val - min_val\n",
        "        rms = np.sqrt(np.mean(np.square(ecg)))\n",
        "\n",
        "        # Detect peaks for QRS complex approximation\n",
        "        # Simple peak detection - in practice use more robust methods\n",
        "        from scipy.signal import find_peaks\n",
        "        peaks, _ = find_peaks(ecg, distance=50)  # Adjust distance based on sampling rate\n",
        "\n",
        "        # Heart rate approximation\n",
        "        if len(peaks) > 1:\n",
        "            rr_intervals = np.diff(peaks)\n",
        "            hr_feature = 60 / (np.mean(rr_intervals) / 250)  # Assuming 250Hz sampling rate\n",
        "            hr_variability = np.std(rr_intervals) if len(rr_intervals) > 1 else 0\n",
        "        else:\n",
        "            hr_feature = 0\n",
        "            hr_variability = 0\n",
        "\n",
        "        # Frequency domain features\n",
        "        fft_vals = np.abs(np.fft.rfft(ecg))\n",
        "        fft_freq = np.fft.rfftfreq(len(ecg))\n",
        "\n",
        "        # Power in different frequency bands\n",
        "        lf_power = np.sum(fft_vals[(fft_freq >= 0.04) & (fft_freq < 0.15)])\n",
        "        hf_power = np.sum(fft_vals[(fft_freq >= 0.15) & (fft_freq < 0.4)])\n",
        "        lf_hf_ratio = lf_power / hf_power if hf_power > 0 else 0\n",
        "\n",
        "        # Dominant frequency\n",
        "        dominant_freq = fft_freq[np.argmax(fft_vals)]\n",
        "\n",
        "        # Wavelet features - energy in different sub-bands\n",
        "        coeffs = pywt.wavedec(ecg, 'db4', level=5)\n",
        "        wavelet_energy = [np.sum(np.square(c)) for c in coeffs]\n",
        "\n",
        "        # Combine all features\n",
        "        feature_vector = np.array([\n",
        "            mean, std, min_val, max_val, p2p, rms,\n",
        "            hr_feature, hr_variability,\n",
        "            lf_power, hf_power, lf_hf_ratio, dominant_freq,\n",
        "            *wavelet_energy\n",
        "        ])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    features = np.array(features)\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "    print(f\"✅ Extracted {features.shape[1]} features for each ECG\")\n",
        "\n",
        "    return features\n",
        "\n",
        "# -------------------- 4. IMPROVED MODEL ARCHITECTURE --------------------\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=15, stride=stride, padding=7, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=15, padding=7, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ECGResNet(nn.Module):\n",
        "    def __init__(self, input_length=3000, num_classes=1):\n",
        "        super(ECGResNet, self).__init__()\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.conv1 = nn.Conv1d(1, 64, kernel_size=15, stride=2, padding=7, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual blocks with increasing channels\n",
        "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "\n",
        "        # Adaptive pooling and fully connected layers\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape if needed: [batch_size, sequence_length] -> [batch_size, 1, sequence_length]\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, input_length=3000, feature_dim=20, num_classes=1):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # Raw signal pathway (ResNet)\n",
        "        self.signal_model = ECGResNet(input_length, 64)  # Output 64 features\n",
        "\n",
        "        # Hand-crafted feature pathway\n",
        "        self.feature_layers = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Combining both pathways\n",
        "        self.combined_layers = nn.Sequential(\n",
        "            nn.Linear(64 + 32, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, signal, features):\n",
        "        # Process raw signal\n",
        "        signal_features = self.signal_model(signal)\n",
        "\n",
        "        # Process hand-crafted features\n",
        "        extracted_features = self.feature_layers(features)\n",
        "\n",
        "        # Combine both sets of features\n",
        "        combined = torch.cat((signal_features, extracted_features), dim=1)\n",
        "\n",
        "        # Final prediction\n",
        "        output = self.combined_layers(combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "# -------------------- 5. LOSS + TRAINING --------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, pos_weight=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Clip inputs to prevent extreme values\n",
        "        inputs = torch.clamp(inputs, -50, 50)\n",
        "\n",
        "        # Using BCE with logits for numerical stability\n",
        "        bce = nn.functional.binary_cross_entropy_with_logits(\n",
        "            inputs, targets,\n",
        "            pos_weight=self.pos_weight,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "        # Safe exponential calculation\n",
        "        pt = torch.exp(-torch.clamp(bce, max=50))\n",
        "\n",
        "        # Calculate focal term with safety checks\n",
        "        focal_term = (1 - pt + 1e-7) ** self.gamma\n",
        "\n",
        "        # Return mean of the loss\n",
        "        loss = focal_term * bce\n",
        "\n",
        "        # Check for NaN and replace with zero\n",
        "        loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, hybrid=False):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for data in train_loader:\n",
        "        if hybrid:\n",
        "            # Hybrid model with signal and features\n",
        "            (X_signal, X_features), y = data\n",
        "            X_signal, X_features = X_signal.to(device), X_features.to(device)\n",
        "            y = y.to(device).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_signal, X_features)\n",
        "        else:\n",
        "            # Regular model with just one input\n",
        "            X, y = data\n",
        "            X, y = X.to(device), y.to(device).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Collect predictions for metrics\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        preds = (probs > 0.5).float()\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    # Calculate training metrics\n",
        "    acc = np.mean((np.array(all_preds) == np.array(all_labels)).astype(float))\n",
        "\n",
        "    return total_loss / len(train_loader), acc\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device, threshold=0.5, hybrid=False):\n",
        "    model.eval()\n",
        "    all_preds, all_probs, all_labels = [], [], []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            if hybrid:\n",
        "                # Hybrid model with signal and features\n",
        "                (X_signal, X_features), y = data\n",
        "                X_signal, X_features = X_signal.to(device), X_features.to(device)\n",
        "                y = y.to(device).unsqueeze(1)\n",
        "\n",
        "                outputs = model(X_signal, X_features)\n",
        "            else:\n",
        "                # Regular model with just one input\n",
        "                X, y = data\n",
        "                X, y = X.to(device), y.to(device).unsqueeze(1)\n",
        "\n",
        "                outputs = model(X)\n",
        "\n",
        "            loss = criterion(outputs, y)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs > threshold).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "    return total_loss / len(val_loader), all_preds, all_labels, auc, all_probs\n",
        "\n",
        "class HybridDataset(Dataset):\n",
        "    def __init__(self, X_signal, X_features, y):\n",
        "        self.X_signal = X_signal\n",
        "        self.X_features = X_features\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        signal = torch.tensor(self.X_signal[idx], dtype=torch.float32)\n",
        "        features = torch.tensor(self.X_features[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.y[idx], dtype=torch.float32)\n",
        "\n",
        "        return (signal, features), label\n",
        "\n",
        "def find_best_threshold(probs, labels):\n",
        "    \"\"\"Find threshold that optimizes F1 score\"\"\"\n",
        "    precisions, recalls, thresholds = precision_recall_curve(labels, probs)\n",
        "\n",
        "    # IMPORTANT: precision_recall_curve returns len(thresholds) = len(precisions) - 1\n",
        "    # So we need to handle this difference carefully\n",
        "\n",
        "    # Calculate F1 scores for different thresholds\n",
        "    f1_scores = []\n",
        "    for i in range(len(thresholds)):\n",
        "        if precisions[i] + recalls[i] > 0:  # Avoid division by zero\n",
        "            f1 = 2 * precisions[i] * recalls[i] / (precisions[i] + recalls[i])\n",
        "            f1_scores.append((thresholds[i], f1, precisions[i], recalls[i]))\n",
        "\n",
        "    if not f1_scores:\n",
        "        print(\"Warning: No valid F1 scores calculated. Using default threshold of 0.5\")\n",
        "        return 0.5\n",
        "\n",
        "    # Find threshold with maximum F1 score\n",
        "    best_threshold, best_f1, best_precision, best_recall = max(f1_scores, key=lambda x: x[1])\n",
        "\n",
        "    print(f\"✅ Best threshold: {best_threshold:.4f} with F1 score: {best_f1:.4f}\")\n",
        "    print(f\"✅ At this threshold - Precision: {best_precision:.4f}, Recall: {best_recall:.4f}\")\n",
        "\n",
        "    return best_threshold\n",
        "\n",
        "def plot_results(val_probs, val_labels, model_name, fold):\n",
        "    \"\"\"Plot and save ROC and Precision-Recall curves\"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # ROC Curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    from sklearn.metrics import roc_curve\n",
        "    fpr, tpr, _ = roc_curve(val_labels, val_probs)\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve (AUC = {roc_auc_score(val_labels, val_probs):.4f})')\n",
        "\n",
        "    # PR Curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    precision, recall, _ = precision_recall_curve(val_labels, val_probs)\n",
        "    plt.plot(recall, precision)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'PR Curve (AP = {average_precision_score(val_labels, val_probs):.4f})')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}_fold{fold}_curves.png')\n",
        "    plt.close()\n",
        "\n",
        "# -------------------- 6. MAIN TRAINING FUNCTION --------------------\n",
        "def run_training(X_signal, X_features, y, use_hybrid=True, n_splits=5, epochs=30, batch_size=64):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    results = []\n",
        "    # Create sampling strategy - balanced for stroke/non-stroke\n",
        "    sampling_strategy = Pipeline([\n",
        "        ('oversample', SMOTE(sampling_strategy=0.5, random_state=42)),\n",
        "        ('undersample', RandomUnderSampler(sampling_strategy=0.8, random_state=42))\n",
        "    ])\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    all_val_probs = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_signal, y)):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"🔄 FOLD {fold+1}/{n_splits}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Split data\n",
        "        X_train_signal, X_test_signal = X_signal[train_idx], X_signal[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        if use_hybrid and X_features is not None:\n",
        "            X_train_features, X_test_features = X_features[train_idx], X_features[test_idx]\n",
        "\n",
        "            # Apply sampling strategy\n",
        "            print(f\"Original training set — Class balance: {Counter(y_train)}\")\n",
        "            combined_train = np.hstack([X_train_signal, X_train_features])\n",
        "            combined_train, y_train = sampling_strategy.fit_resample(combined_train, y_train)\n",
        "            X_train_signal = combined_train[:, :X_train_signal.shape[1]]\n",
        "            X_train_features = combined_train[:, X_train_signal.shape[1]:]\n",
        "            print(f\"✅ Resampled training set — Class balance: {Counter(y_train)}\")\n",
        "\n",
        "            # Create hybrid dataloaders\n",
        "            train_dataset = HybridDataset(X_train_signal, X_train_features, y_train)\n",
        "            test_dataset = HybridDataset(X_test_signal, X_test_features, y_test)\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "            # Create hybrid model\n",
        "            feature_dim = X_train_features.shape[1]\n",
        "            model = HybridModel(X_train_signal.shape[1], feature_dim).to(device)\n",
        "            model_name = \"hybrid_model\"\n",
        "        else:\n",
        "            # Apply sampling strategy to signal data only\n",
        "            print(f\"Original training set — Class balance: {Counter(y_train)}\")\n",
        "            X_train_signal, y_train = sampling_strategy.fit_resample(X_train_signal, y_train)\n",
        "            print(f\"✅ Resampled training set — Class balance: {Counter(y_train)}\")\n",
        "\n",
        "            # Create standard dataloaders\n",
        "            train_dataset = ECGDataset(X_train_signal, y_train)\n",
        "            test_dataset = ECGDataset(X_test_signal, y_test)\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "            # Create ResNet model\n",
        "            model = ECGResNet(X_train_signal.shape[1]).to(device)\n",
        "            model_name = \"resnet_model\"\n",
        "\n",
        "        # Set up loss function with class weighting\n",
        "        pos_count = np.sum(y_train)\n",
        "        neg_count = len(y_train) - pos_count\n",
        "        pos_weight = torch.tensor([neg_count / pos_count]).to(device)\n",
        "\n",
        "        criterion = FocalLoss(gamma=2.0, pos_weight=pos_weight)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6)\n",
        "\n",
        "        # Training tracking\n",
        "        best_auc = 0\n",
        "        best_model = None\n",
        "        best_val_probs = None\n",
        "        best_val_labels = None\n",
        "        patience = 5\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Train\n",
        "            train_loss, train_acc = train_epoch(\n",
        "                model, train_loader, criterion, optimizer, device, hybrid=use_hybrid\n",
        "            )\n",
        "\n",
        "            # Evaluate\n",
        "            val_loss, val_preds, val_labels, val_auc, val_probs = evaluate(\n",
        "                model, test_loader, criterion, device, threshold=0.5, hybrid=use_hybrid\n",
        "            )\n",
        "\n",
        "            # Update scheduler\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_auc > best_auc:\n",
        "                best_auc = val_auc\n",
        "                best_model = copy.deepcopy(model)\n",
        "                best_val_probs = val_probs\n",
        "                best_val_labels = val_labels\n",
        "                patience_counter = 0\n",
        "                print(f\"✅ New best model saved! AUC: {val_auc:.4f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Store validation results for threshold optimization\n",
        "        all_val_probs.extend(best_val_probs)\n",
        "        all_val_labels.extend(best_val_labels)\n",
        "\n",
        "        # Plot validation results\n",
        "        plot_results(best_val_probs, best_val_labels, model_name, fold+1)\n",
        "\n",
        "        # Find best threshold\n",
        "        best_threshold = find_best_threshold(np.array(best_val_probs), np.array(best_val_labels))\n",
        "\n",
        "        # Final evaluation with best model and threshold\n",
        "        _, final_preds, final_labels, final_auc, _ = evaluate(\n",
        "            best_model, test_loader, criterion, device, threshold=best_threshold, hybrid=use_hybrid\n",
        "        )\n",
        "\n",
        "        print(\"\\n📊 Final Evaluation Results:\")\n",
        "        print(f\"→ ROC-AUC: {final_auc:.4f}\")\n",
        "        print(f\"→ Predicted Positives: {int(final_preds.sum())} / {len(final_preds)}\")\n",
        "        print(f\"→ Actual Positives: {int(final_labels.sum())} / {len(final_labels)}\")\n",
        "\n",
        "        # Classification report\n",
        "        report = classification_report(\n",
        "            final_labels, final_preds,\n",
        "            target_names=['Non-Stroke', 'Stroke'],\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        # Save results\n",
        "        results.append({\n",
        "            'fold': fold+1,\n",
        "            'auc': final_auc,\n",
        "            'threshold': best_threshold,\n",
        "            'report': report,\n",
        "            'model': best_model\n",
        "        })\n",
        "\n",
        "        # Save model checkpoint\n",
        "        torch.save({\n",
        "            'model_state_dict': best_model.state_dict(),\n",
        "            'threshold': best_threshold,\n",
        "            'auc': final_auc,\n",
        "            'hybrid': use_hybrid,\n",
        "        }, f'{model_name}_fold{fold+1}.pt')\n",
        "        print(f\"✅ Model saved to {model_name}_fold{fold+1}.pt\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------- 7. PREDICTION WITH ENSEMBLE (CONTINUED) --------------------\n",
        "def predict_with_ensemble(X_signal, X_features=None, model_paths=None, threshold=None):\n",
        "    \"\"\"\n",
        "    Make predictions using an ensemble of models\n",
        "\n",
        "    Args:\n",
        "        X_signal: ECG signal data\n",
        "        X_features: Optional extracted features if using hybrid models\n",
        "        model_paths: List of paths to model checkpoints\n",
        "        threshold: Optional prediction threshold (if None, uses saved threshold)\n",
        "\n",
        "    Returns:\n",
        "        Average probabilities and binary predictions\n",
        "    \"\"\"\n",
        "    if not model_paths:\n",
        "        # Default - try to find all models\n",
        "        model_paths = [f for f in os.listdir('.') if f.endswith('.pt')]\n",
        "        if not model_paths:\n",
        "            raise ValueError(\"No model checkpoints found!\")\n",
        "        print(f\"Found {len(model_paths)} model checkpoints\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    all_probs = []\n",
        "\n",
        "    for model_path in model_paths:\n",
        "        try:\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "            is_hybrid = checkpoint.get('hybrid', False)\n",
        "\n",
        "            if is_hybrid and X_features is None:\n",
        "                print(f\"Warning: {model_path} is a hybrid model but no features provided. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Create appropriate model\n",
        "            if is_hybrid:\n",
        "                model = HybridModel(X_signal.shape[1], X_features.shape[1]).to(device)\n",
        "            else:\n",
        "                model = ECGResNet(X_signal.shape[1]).to(device)\n",
        "\n",
        "            # Load weights\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model.eval()\n",
        "\n",
        "            # Use the model's saved threshold if none provided\n",
        "            model_threshold = threshold if threshold is not None else checkpoint.get('threshold', 0.5)\n",
        "\n",
        "            # Prepare data\n",
        "            if is_hybrid:\n",
        "                dataset = HybridDataset(X_signal, X_features, np.zeros(len(X_signal)))  # Dummy labels\n",
        "            else:\n",
        "                dataset = ECGDataset(X_signal, np.zeros(len(X_signal)))  # Dummy labels\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=64)\n",
        "\n",
        "            # Get predictions\n",
        "            probs = []\n",
        "            with torch.no_grad():\n",
        "                for data in loader:\n",
        "                    if is_hybrid:\n",
        "                        (X_sig, X_feat), _ = data\n",
        "                        X_sig, X_feat = X_sig.to(device), X_feat.to(device)\n",
        "                        outputs = model(X_sig, X_feat)\n",
        "                    else:\n",
        "                        X, _ = data\n",
        "                        X = X.to(device)\n",
        "                        outputs = model(X)\n",
        "\n",
        "                    batch_probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "                    probs.extend(batch_probs)\n",
        "\n",
        "            all_probs.append(np.array(probs))\n",
        "            print(f\"✅ Generated predictions with model: {model_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {model_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_probs:\n",
        "        raise ValueError(\"No valid predictions were generated from any model!\")\n",
        "\n",
        "    # Average probabilities from all models\n",
        "    all_probs = [p.reshape(-1) for p in all_probs]  # Ensure all are 1D arrays\n",
        "    avg_probs = np.mean(np.array(all_probs), axis=0)\n",
        "\n",
        "    # Use threshold to get binary predictions\n",
        "    final_threshold = threshold if threshold is not None else 0.5\n",
        "    binary_preds = (avg_probs > final_threshold).astype(int)\n",
        "\n",
        "    print(f\"✅ Ensemble predictions complete: {sum(binary_preds)} positives out of {len(binary_preds)}\")\n",
        "    return avg_probs, binary_preds\n",
        "\n",
        "# -------------------- 8. VISUALIZATION UTILITIES --------------------\n",
        "def visualize_ecg_with_prediction(ecg_signal, prediction, probability, true_label=None, idx=0):\n",
        "    \"\"\"Visualize an ECG signal with its prediction\"\"\"\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(ecg_signal)\n",
        "\n",
        "    # Add prediction information\n",
        "    title = f\"ECG #{idx} - Prediction: {'Stroke' if prediction else 'Non-Stroke'} (Prob: {probability:.4f})\"\n",
        "    if true_label is not None:\n",
        "        title += f\" - True: {'Stroke' if true_label else 'Non-Stroke'}\"\n",
        "    plt.title(title)\n",
        "\n",
        "    # Highlight based on prediction\n",
        "    if prediction:\n",
        "        plt.axhspan(min(ecg_signal), max(ecg_signal), alpha=0.2, color='red')\n",
        "\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return plt.gcf()  # Return the figure\n",
        "\n",
        "def visualize_feature_importance(model, feature_names):\n",
        "    \"\"\"Analyze feature importance for the hybrid model\"\"\"\n",
        "    # Only works for the hybrid model linear layers\n",
        "    if not hasattr(model, 'feature_layers'):\n",
        "        print(\"Feature importance visualization only works with hybrid models\")\n",
        "        return None\n",
        "\n",
        "    # Extract weights from the first linear layer\n",
        "    weights = model.feature_layers[0].weight.detach().cpu().numpy()\n",
        "\n",
        "    # Calculate absolute importance\n",
        "    importance = np.mean(np.abs(weights), axis=0)\n",
        "\n",
        "    # Sort features by importance\n",
        "    sorted_idx = np.argsort(importance)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(range(len(sorted_idx)), importance[sorted_idx])\n",
        "    plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title('ECG Feature Importance for Stroke Prediction')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig('feature_importance.png')\n",
        "    print(\"✅ Feature importance plot saved as 'feature_importance.png'\")\n",
        "    return plt.gcf()\n",
        "\n",
        "# -------------------- 9. EXECUTION --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Starting Enhanced ECG Stroke Prediction System\")\n",
        "\n",
        "    # 1. Load raw data\n",
        "    ecg_data = load_ecg_data()\n",
        "    diagnosis_df = load_diagnosis_data(\"records_w_diag_icd10 (1).csv\", len(ecg_data))\n",
        "    diagnosis_df = create_stroke_labels(diagnosis_df)\n",
        "\n",
        "    # 2. Print class distribution\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    class_counts = diagnosis_df[\"stroke_label\"].value_counts()\n",
        "    print(class_counts)\n",
        "    print(f\"Stroke prevalence: {class_counts[1]/len(diagnosis_df):.4%}\")\n",
        "\n",
        "    # 3. Preprocess ECG data\n",
        "    ecg_data_processed = preprocess_ecg(ecg_data, wavelet_transform=True)\n",
        "\n",
        "    # 4. Extract features\n",
        "    ecg_features = extract_ecg_features(ecg_data_processed)\n",
        "\n",
        "    # 5. Run cross-validation with the hybrid model\n",
        "    print(\"\\n🔄 Starting cross-validation training with hybrid model...\")\n",
        "    hybrid_results = run_training(\n",
        "        ecg_data_processed,\n",
        "        ecg_features,\n",
        "        diagnosis_df[\"stroke_label\"].values,\n",
        "        use_hybrid=True,\n",
        "        n_splits=5,\n",
        "        epochs=10,\n",
        "        batch_size=64\n",
        "    )\n",
        "\n",
        "    # 6. Run cross-validation with the ResNet model only for comparison\n",
        "    print(\"\\n🔄 Starting cross-validation training with ResNet model only...\")\n",
        "    resnet_results = run_training(\n",
        "        ecg_data_processed,\n",
        "        None,  # No features\n",
        "        diagnosis_df[\"stroke_label\"].values,\n",
        "        use_hybrid=False,\n",
        "        n_splits=5,\n",
        "        epochs=10,\n",
        "        batch_size=64\n",
        "    )\n",
        "\n",
        "    # 7. Compare results\n",
        "    print(\"\\n📊 Comparing Model Performance:\")\n",
        "    print(\"\\nHybrid Model Results:\")\n",
        "    hybrid_aucs = []\n",
        "    for res in hybrid_results:\n",
        "        print(f\"Fold {res['fold']} - AUC: {res['auc']:.4f}, Best Threshold: {res['threshold']:.4f}\")\n",
        "        hybrid_aucs.append(res['auc'])\n",
        "\n",
        "        if 'Stroke' in res['report']:\n",
        "            print(f\"Stroke Precision: {res['report']['Stroke']['precision']:.4f}\")\n",
        "            print(f\"Stroke Recall: {res['report']['Stroke']['recall']:.4f}\")\n",
        "            print(f\"Stroke F1 Score: {res['report']['Stroke']['f1-score']:.4f}\")\n",
        "        elif '1' in res['report']:\n",
        "            print(f\"Stroke Precision: {res['report']['1']['precision']:.4f}\")\n",
        "            print(f\"Stroke Recall: {res['report']['1']['recall']:.4f}\")\n",
        "            print(f\"Stroke F1 Score: {res['report']['1']['f1-score']:.4f}\")\n",
        "\n",
        "    print(f\"\\nHybrid Model Average AUC: {np.mean(hybrid_aucs):.4f}\")\n",
        "\n",
        "    print(\"\\nResNet Model Results:\")\n",
        "    resnet_aucs = []\n",
        "    for res in resnet_results:\n",
        "        print(f\"Fold {res['fold']} - AUC: {res['auc']:.4f}, Best Threshold: {res['threshold']:.4f}\")\n",
        "        resnet_aucs.append(res['auc'])\n",
        "\n",
        "        if 'Stroke' in res['report']:\n",
        "            print(f\"Stroke Precision: {res['report']['Stroke']['precision']:.4f}\")\n",
        "            print(f\"Stroke Recall: {res['report']['Stroke']['recall']:.4f}\")\n",
        "            print(f\"Stroke F1 Score: {res['report']['Stroke']['f1-score']:.4f}\")\n",
        "        elif '1' in res['report']:\n",
        "            print(f\"Stroke Precision: {res['report']['1']['precision']:.4f}\")\n",
        "            print(f\"Stroke Recall: {res['report']['1']['recall']:.4f}\")\n",
        "            print(f\"Stroke F1 Score: {res['report']['1']['f1-score']:.4f}\")\n",
        "\n",
        "    print(f\"\\nResNet Model Average AUC: {np.mean(resnet_aucs):.4f}\")\n",
        "\n",
        "    # 8. Create and save final ensemble model prediction\n",
        "    hybrid_models = [f for f in os.listdir('.') if f.startswith('hybrid_model') and f.endswith('.pt')]\n",
        "    resnet_models = [f for f in os.listdir('.') if f.startswith('resnet_model') and f.endswith('.pt')]\n",
        "\n",
        "    # Use the model with better performance for final predictions\n",
        "    if np.mean(hybrid_aucs) > np.mean(resnet_aucs):\n",
        "        print(\"\\n🔍 Using Hybrid models for final ensemble prediction\")\n",
        "        ensemble_probs, ensemble_preds = predict_with_ensemble(\n",
        "            ecg_data_processed,\n",
        "            X_features=ecg_features,\n",
        "            model_paths=hybrid_models\n",
        "        )\n",
        "        best_model_type = \"hybrid\"\n",
        "    else:\n",
        "        print(\"\\n🔍 Using ResNet models for final ensemble prediction\")\n",
        "        ensemble_probs, ensemble_preds = predict_with_ensemble(\n",
        "            ecg_data_processed,\n",
        "            model_paths=resnet_models\n",
        "        )\n",
        "        best_model_type = \"resnet\"\n",
        "\n",
        "    # 9. Save final predictions to CSV\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'ecg_id': range(len(ecg_data_processed)),\n",
        "        'stroke_probability': ensemble_probs,\n",
        "        'stroke_prediction': ensemble_preds\n",
        "    })\n",
        "\n",
        "    if 'stroke_label' in diagnosis_df.columns:\n",
        "        predictions_df['true_label'] = diagnosis_df['stroke_label'].values\n",
        "\n",
        "    predictions_df.to_csv('stroke_predictions.csv', index=False)\n",
        "    print(\"✅ Final predictions saved to 'stroke_predictions.csv'\")\n",
        "\n",
        "    # 10. Visualize some examples\n",
        "    print(\"\\n🖼️ Generating example visualizations...\")\n",
        "    if 'true_label' in predictions_df.columns:\n",
        "        # Plot some true positives, false positives, true negatives, false negatives\n",
        "        tp_idx = predictions_df[(predictions_df['stroke_prediction'] == 1) &\n",
        "                               (predictions_df['true_label'] == 1)].index[:3]\n",
        "        fp_idx = predictions_df[(predictions_df['stroke_prediction'] == 1) &\n",
        "                               (predictions_df['true_label'] == 0)].index[:3]\n",
        "        tn_idx = predictions_df[(predictions_df['stroke_prediction'] == 0) &\n",
        "                               (predictions_df['true_label'] == 0)].index[:3]\n",
        "        fn_idx = predictions_df[(predictions_df['stroke_prediction'] == 0) &\n",
        "                               (predictions_df['true_label'] == 1)].index[:3]\n",
        "\n",
        "        for i, idx in enumerate(tp_idx):\n",
        "            fig = visualize_ecg_with_prediction(\n",
        "                ecg_data_processed[idx], 1,\n",
        "                predictions_df.loc[idx, 'stroke_probability'],\n",
        "                true_label=1, idx=idx\n",
        "            )\n",
        "            fig.savefig(f'example_tp_{i}.png')\n",
        "\n",
        "        for i, idx in enumerate(fp_idx):\n",
        "            fig = visualize_ecg_with_prediction(\n",
        "                ecg_data_processed[idx], 1,\n",
        "                predictions_df.loc[idx, 'stroke_probability'],\n",
        "                true_label=0, idx=idx\n",
        "            )\n",
        "            fig.savefig(f'example_fp_{i}.png')\n",
        "\n",
        "        for i, idx in enumerate(fn_idx):\n",
        "            fig = visualize_ecg_with_prediction(\n",
        "                ecg_data_processed[idx], 0,\n",
        "                predictions_df.loc[idx, 'stroke_probability'],\n",
        "                true_label=1, idx=idx\n",
        "            )\n",
        "            fig.savefig(f'example_fn_{i}.png')\n",
        "\n",
        "    # 11. If hybrid model was better, visualize feature importance\n",
        "    if best_model_type == \"hybrid\":\n",
        "        print(\"\\n📊 Visualizing feature importance...\")\n",
        "\n",
        "        # Define feature names\n",
        "        feature_names = [\n",
        "            \"Mean\", \"Std Dev\", \"Min Value\", \"Max Value\", \"Peak-to-Peak\",\n",
        "            \"RMS\", \"Heart Rate\", \"HR Variability\", \"LF Power\", \"HF Power\",\n",
        "            \"LF/HF Ratio\", \"Dominant Freq\", \"Wavelet Energy L1\", \"Wavelet Energy L2\",\n",
        "            \"Wavelet Energy L3\", \"Wavelet Energy L4\", \"Wavelet Energy L5\", \"Wavelet Energy L6\"\n",
        "        ]\n",
        "\n",
        "        # Pad with additional features if needed\n",
        "        if len(feature_names) < ecg_features.shape[1]:\n",
        "            for i in range(len(feature_names), ecg_features.shape[1]):\n",
        "                feature_names.append(f\"Feature_{i+1}\")\n",
        "\n",
        "        # Visualize feature importance for the first hybrid model\n",
        "        best_model = torch.load(hybrid_models[0], map_location=device)['model']\n",
        "        visualize_feature_importance(best_model, feature_names)\n",
        "\n",
        "    print(\"\\n✅ ECG Stroke Prediction System Completed Successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYkDZoPBcK-z",
        "outputId": "ea715eea-b9e9-4a35-a834-4dcfd4dae9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n",
            "🚀 Starting Enhanced ECG Stroke Prediction System\n",
            "✅ Loaded ECG data from .npy file\n",
            "Shape: (86596, 3000)\n",
            "\n",
            "🔍 Loading diagnosis data from records_w_diag_icd10 (1).csv\n",
            "Aligned to 86596 rows to match ECG data\n",
            "\n",
            "Class Distribution:\n",
            "stroke_label\n",
            "0    83366\n",
            "1     3230\n",
            "Name: count, dtype: int64\n",
            "Stroke prevalence: 3.7300%\n",
            "Preprocessing ECG data...\n",
            "Applying wavelet denoising...\n",
            "Standardizing signals...\n",
            "✅ ECG preprocessing complete\n",
            "Extracting ECG features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 86596/86596 [00:43<00:00, 2013.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted 18 features for each ECG\n",
            "\n",
            "🔄 Starting cross-validation training with hybrid model...\n",
            "Using device: cuda\n",
            "\n",
            "==================================================\n",
            "🔄 FOLD 1/5\n",
            "==================================================\n",
            "Original training set — Class balance: Counter({np.int64(0): 66692, np.int64(1): 2584})\n",
            "✅ Resampled training set — Class balance: Counter({np.int64(0): 41682, np.int64(1): 33346})\n",
            "Epoch 1/10 | Train Loss: 0.1223 | Train Acc: 0.8474 | Val Loss: 0.1527 | Val AUC: 0.5416\n",
            "✅ New best model saved! AUC: 0.5416\n",
            "Epoch 2/10 | Train Loss: 0.1050 | Train Acc: 0.8773 | Val Loss: 0.0678 | Val AUC: 0.5440\n",
            "✅ New best model saved! AUC: 0.5440\n",
            "Epoch 3/10 | Train Loss: 0.0989 | Train Acc: 0.8866 | Val Loss: 0.3799 | Val AUC: 0.5407\n",
            "Epoch 4/10 | Train Loss: 0.0947 | Train Acc: 0.8932 | Val Loss: 0.0651 | Val AUC: 0.5632\n",
            "✅ New best model saved! AUC: 0.5632\n",
            "Epoch 5/10 | Train Loss: 0.0903 | Train Acc: 0.9009 | Val Loss: 0.0991 | Val AUC: 0.5567\n",
            "Epoch 6/10 | Train Loss: 0.0863 | Train Acc: 0.9059 | Val Loss: 0.0899 | Val AUC: 0.5744\n",
            "✅ New best model saved! AUC: 0.5744\n",
            "Epoch 7/10 | Train Loss: 0.0812 | Train Acc: 0.9135 | Val Loss: 0.1170 | Val AUC: 0.5846\n",
            "✅ New best model saved! AUC: 0.5846\n",
            "Epoch 8/10 | Train Loss: 0.0760 | Train Acc: 0.9195 | Val Loss: 0.0625 | Val AUC: 0.5859\n",
            "✅ New best model saved! AUC: 0.5859\n",
            "Epoch 9/10 | Train Loss: 0.0706 | Train Acc: 0.9261 | Val Loss: 0.0716 | Val AUC: 0.6006\n",
            "✅ New best model saved! AUC: 0.6006\n",
            "Epoch 10/10 | Train Loss: 0.0664 | Train Acc: 0.9307 | Val Loss: 0.0761 | Val AUC: 0.6034\n",
            "✅ New best model saved! AUC: 0.6034\n",
            "✅ Best threshold: 0.4158 with F1 score: 0.1213\n",
            "✅ At this threshold - Precision: 0.0857, Recall: 0.2074\n",
            "\n",
            "📊 Final Evaluation Results:\n",
            "→ ROC-AUC: 0.6034\n",
            "→ Predicted Positives: 1562 / 17320\n",
            "→ Actual Positives: 646 / 17320\n",
            "✅ Model saved to hybrid_model_fold1.pt\n",
            "\n",
            "==================================================\n",
            "🔄 FOLD 2/5\n",
            "==================================================\n",
            "Original training set — Class balance: Counter({np.int64(0): 66693, np.int64(1): 2584})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mNHqxcj1voV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}